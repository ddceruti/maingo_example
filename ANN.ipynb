{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80e7fb5",
   "metadata": {},
   "source": [
    "# Example ANN generation and training\n",
    "\n",
    "From melon doc (https://git.rwth-aachen.de/avt-svt/public/MeLOn/-/blob/master/feedforward%20neural%20network/training/keras/example_training_of_ANN.py) and the ANN paper (https://link.springer.com/content/pdf/10.1007/s10957-018-1396-0.pdf).\n",
    "\n",
    "## Problem statement:\n",
    "min $f(x)$\n",
    "\n",
    "where $f$ is a feed forward neural network. \n",
    "\n",
    "The peaks function will be used as a dummy problem and will be approximated by $f$. The peaks function is defined as:\n",
    "\n",
    "$$ g_{peaks}(x_1, x_2) = 3\\cdot(1-x_1)^2 \\cdot \\exp[-x_1^2-(x_2+1)^2] -10\\cdot (x_1/5-x_1^3-x_2^5)\\cdot\\exp(-x_1^2-x_2^2)- \\exp[-(x_1+1)^2-x_2^2]/3$$\n",
    "\n",
    "$$g_{peaks} : \\mathbb{R}^2 \\to \\mathbb{R}$$\n",
    "\n",
    "The function dominion is defined as $D = \\{x_1, x_2 \\in \\mathbb{R}: -3 \\leq x_1,x_2 \\leq 3 \\}$\n",
    "\n",
    "The initial dataset will be generated by Latin hypercube sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ace0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE import lhs\n",
    "import maingopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d932c",
   "metadata": {},
   "source": [
    "Implement peaks function as defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971eea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks(X):\n",
    "    term1 = 3*np.multiply((1-X[:,0])**2, np.exp(-(X[:,0])**2-(X[:,1]+1)**2))\n",
    "    term2 = np.multiply(-10*(X[:,0]/5-X[:,0]**3-X[:,1]**5), np.exp(-X[:,0]**2-X[:,1]**2))\n",
    "    term3 = -np.exp(-(X[:,0]+1)**2-X[:,1]**2)/3\n",
    "    y = term1 + term2 + term3\n",
    "    return np.expand_dims(y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd87cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.55112995],\n",
       "       [ 0.37537558]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test global minimum at f(0.228, -1.626)=-6.551\n",
    "peaks(np.array([[0.228, -1.626], [0.5,0.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c713a",
   "metadata": {},
   "source": [
    "## Training data generation\n",
    "\n",
    "Generate 500 samples with latin hypercube sampling (package pyDOE), rescale to [-3, 3] and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7439d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhd = lhs(2, samples=600)\n",
    "Xpeaks = lhd*6-3\n",
    "ypeaks = peaks(Xpeaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd4d16",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d3211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_name = 'peaks'\n",
    "# dimensionality of the data\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "# scale Input to [-1,1] range\n",
    "scaleInput = True\n",
    "# normalize Output to z-score\n",
    "normalizeOutput = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa483e8",
   "metadata": {},
   "source": [
    "define scale and normalize (from melon keras utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0023b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, scaleInput):\n",
    "    # scale Input values to range [-1,1] in each dimension\n",
    "    if (scaleInput):\n",
    "        nom = (X -  X.min(axis=0))*2\n",
    "        denom = X.max(axis=0) - X.min(axis=0)\n",
    "        denom[denom==0] = 1\n",
    "        return -1 + nom/denom\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def normalize(y, normalizeOutput):\n",
    "    # normalize output to z-score\n",
    "    if(normalizeOutput):\n",
    "        y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0);\n",
    "        return y_norm\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0a9d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnorm = scale(Xpeaks, scaleInput)\n",
    "ynorm = normalize(ypeaks, normalizeOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca326bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test, validation and test sets (70-15-15% split)\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(Xnorm, ynorm, test_size=0.15)\n",
    "n_train = Xtrain.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df241dc",
   "metadata": {},
   "source": [
    "## Set output parameters\n",
    "\n",
    "Two hidden layers of 10, 8 (different from source paper)-> (2x10+1) + (10x8+1) + (8x1)+1 = 127 parameters to optimize.\n",
    "\n",
    "ReLU activation function for the all activation functions exctept the output layer, which is linear. \n",
    "\n",
    "Learning by first-order stochastic gradient descent (ADAM, https://arxiv.org/abs/1412.6980) for 100 epochs with a 0.001 leraning rate (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa061dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output filename\n",
    "output_folder = \"./data/Output/\"\n",
    "filename_out = output_folder + problem_name\n",
    "# training parameters\n",
    "network_layout = [10, 8]\n",
    "activation_function = 'relu'\n",
    "activation_function_out = 'linear'\n",
    "learning_rate = 0.001\n",
    "kernel_regularizer = tf.keras.regularizers.l2(l=0.0001)  # L2 regularization penalty\n",
    "# 'he_normal' for relu activation, 'glorot_uniform' for everything else\n",
    "kernel_initializer = 'he_normal'\n",
    "# It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.\n",
    "optimizer = 'adam'\n",
    "epochs = 1000\n",
    "#batch_size = 128\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289ac5b",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "With the inputs defined above:\n",
    "\n",
    "keras.sequential: a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a5c6881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (Dense)               (None, 10)                30        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 88        \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 127\n",
      "Trainable params: 127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential class: Linear stack of layers.\n",
    "model = tf.keras.Sequential()\n",
    "# Create and add first layer\n",
    "model.add(tf.keras.layers.Dense(network_layout[0],\n",
    "                                name=\"input\",\n",
    "                                kernel_initializer=kernel_initializer,\n",
    "                                kernel_regularizer=kernel_regularizer,\n",
    "                                activation=activation_function,\n",
    "                                input_dim=input_dim))\n",
    "# Create and add all remaining layers (in this case, 9x9 layout)\n",
    "for neuron in network_layout[1:]:\n",
    "    model.add(tf.keras.layers.Dense(neuron,\n",
    "                                    kernel_initializer=kernel_initializer,\n",
    "                                    kernel_regularizer=kernel_regularizer,\n",
    "                                    activation=activation_function))\n",
    "# Output layer w linear function\n",
    "model.add(tf.keras.layers.Dense(output_dim, name=\"output\",\n",
    "                                kernel_initializer='glorot_uniform',\n",
    "                                kernel_regularizer=kernel_regularizer,\n",
    "                                activation=activation_function_out))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "# Generate a table summarizing the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a904b",
   "metadata": {},
   "source": [
    "## Train ANN model on dataset\n",
    "\n",
    "First-order gradient-based optimization of stochastic objective functions (https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50507067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "16/16 [==============================] - 1s 13ms/step - loss: 1.3259 - mse: 1.3212 - mae: 0.8064 - val_loss: 1.0948 - val_mse: 1.0901 - val_mae: 0.7542\n",
      "Epoch 2/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.1635 - mse: 1.1588 - mae: 0.7339 - val_loss: 0.9385 - val_mse: 0.9338 - val_mae: 0.6865\n",
      "Epoch 3/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0768 - mse: 1.0721 - mae: 0.6871 - val_loss: 0.8556 - val_mse: 0.8509 - val_mae: 0.6459\n",
      "Epoch 4/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 1.0264 - mse: 1.0218 - mae: 0.6599 - val_loss: 0.8112 - val_mse: 0.8065 - val_mae: 0.6211\n",
      "Epoch 5/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9971 - mse: 0.9925 - mae: 0.6451 - val_loss: 0.7780 - val_mse: 0.7734 - val_mae: 0.6089\n",
      "Epoch 6/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9736 - mse: 0.9690 - mae: 0.6389 - val_loss: 0.7583 - val_mse: 0.7537 - val_mae: 0.6024\n",
      "Epoch 7/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9562 - mse: 0.9517 - mae: 0.6359 - val_loss: 0.7448 - val_mse: 0.7402 - val_mae: 0.5984\n",
      "Epoch 8/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9422 - mse: 0.9377 - mae: 0.6336 - val_loss: 0.7307 - val_mse: 0.7261 - val_mae: 0.5919\n",
      "Epoch 9/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9288 - mse: 0.9242 - mae: 0.6302 - val_loss: 0.7199 - val_mse: 0.7154 - val_mae: 0.5872\n",
      "Epoch 10/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9158 - mse: 0.9113 - mae: 0.6245 - val_loss: 0.7096 - val_mse: 0.7050 - val_mae: 0.5809\n",
      "Epoch 11/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.9049 - mse: 0.9003 - mae: 0.6210 - val_loss: 0.6975 - val_mse: 0.6930 - val_mae: 0.5756\n",
      "Epoch 12/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8915 - mse: 0.8870 - mae: 0.6141 - val_loss: 0.6904 - val_mse: 0.6858 - val_mae: 0.5711\n",
      "Epoch 13/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8791 - mse: 0.8745 - mae: 0.6084 - val_loss: 0.6813 - val_mse: 0.6767 - val_mae: 0.5668\n",
      "Epoch 14/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8659 - mse: 0.8613 - mae: 0.6049 - val_loss: 0.6672 - val_mse: 0.6626 - val_mae: 0.5605\n",
      "Epoch 15/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8521 - mse: 0.8475 - mae: 0.6018 - val_loss: 0.6561 - val_mse: 0.6515 - val_mae: 0.5571\n",
      "Epoch 16/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8390 - mse: 0.8343 - mae: 0.5986 - val_loss: 0.6432 - val_mse: 0.6385 - val_mae: 0.5502\n",
      "Epoch 17/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8243 - mse: 0.8196 - mae: 0.5938 - val_loss: 0.6337 - val_mse: 0.6291 - val_mae: 0.5467\n",
      "Epoch 18/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.8101 - mse: 0.8054 - mae: 0.5894 - val_loss: 0.6208 - val_mse: 0.6161 - val_mae: 0.5393\n",
      "Epoch 19/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.7915 - mae: 0.5865 - val_loss: 0.6078 - val_mse: 0.6031 - val_mae: 0.5329\n",
      "Epoch 20/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7818 - mse: 0.7770 - mae: 0.5834 - val_loss: 0.5976 - val_mse: 0.5929 - val_mae: 0.5282\n",
      "Epoch 21/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7694 - mse: 0.7646 - mae: 0.5815 - val_loss: 0.5876 - val_mse: 0.5828 - val_mae: 0.5214\n",
      "Epoch 22/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7554 - mse: 0.7507 - mae: 0.5764 - val_loss: 0.5765 - val_mse: 0.5717 - val_mae: 0.5150\n",
      "Epoch 23/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7428 - mse: 0.7380 - mae: 0.5735 - val_loss: 0.5692 - val_mse: 0.5644 - val_mae: 0.5109\n",
      "Epoch 24/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7306 - mse: 0.7257 - mae: 0.5699 - val_loss: 0.5565 - val_mse: 0.5517 - val_mae: 0.5050\n",
      "Epoch 25/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7185 - mse: 0.7136 - mae: 0.5696 - val_loss: 0.5490 - val_mse: 0.5441 - val_mae: 0.5012\n",
      "Epoch 26/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7078 - mse: 0.7029 - mae: 0.5668 - val_loss: 0.5418 - val_mse: 0.5369 - val_mae: 0.4990\n",
      "Epoch 27/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6973 - mse: 0.6924 - mae: 0.5645 - val_loss: 0.5368 - val_mse: 0.5318 - val_mae: 0.4954\n",
      "Epoch 28/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6872 - mse: 0.6822 - mae: 0.5588 - val_loss: 0.5271 - val_mse: 0.5221 - val_mae: 0.4883\n",
      "Epoch 29/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6768 - mse: 0.6718 - mae: 0.5586 - val_loss: 0.5205 - val_mse: 0.5155 - val_mae: 0.4889\n",
      "Epoch 30/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6659 - mse: 0.6609 - mae: 0.5569 - val_loss: 0.5172 - val_mse: 0.5121 - val_mae: 0.4893\n",
      "Epoch 31/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6551 - mse: 0.6500 - mae: 0.5524 - val_loss: 0.5073 - val_mse: 0.5023 - val_mae: 0.4826\n",
      "Epoch 32/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6383 - mse: 0.6332 - mae: 0.5466 - val_loss: 0.4993 - val_mse: 0.4942 - val_mae: 0.4756\n",
      "Epoch 33/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6235 - mse: 0.6184 - mae: 0.5423 - val_loss: 0.4891 - val_mse: 0.4840 - val_mae: 0.4698\n",
      "Epoch 34/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6112 - mse: 0.6060 - mae: 0.5400 - val_loss: 0.4832 - val_mse: 0.4780 - val_mae: 0.4651\n",
      "Epoch 35/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5986 - mse: 0.5934 - mae: 0.5331 - val_loss: 0.4798 - val_mse: 0.4746 - val_mae: 0.4619\n",
      "Epoch 36/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5880 - mse: 0.5828 - mae: 0.5306 - val_loss: 0.4731 - val_mse: 0.4679 - val_mae: 0.4593\n",
      "Epoch 37/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5804 - mse: 0.5752 - mae: 0.5297 - val_loss: 0.4688 - val_mse: 0.4635 - val_mae: 0.4592\n",
      "Epoch 38/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5724 - mse: 0.5671 - mae: 0.5297 - val_loss: 0.4647 - val_mse: 0.4594 - val_mae: 0.4561\n",
      "Epoch 39/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5643 - mse: 0.5590 - mae: 0.5235 - val_loss: 0.4603 - val_mse: 0.4550 - val_mae: 0.4506\n",
      "Epoch 40/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5579 - mse: 0.5525 - mae: 0.5204 - val_loss: 0.4602 - val_mse: 0.4549 - val_mae: 0.4548\n",
      "Epoch 41/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5503 - mse: 0.5449 - mae: 0.5181 - val_loss: 0.4568 - val_mse: 0.4514 - val_mae: 0.4518\n",
      "Epoch 42/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5451 - mse: 0.5397 - mae: 0.5151 - val_loss: 0.4526 - val_mse: 0.4472 - val_mae: 0.4480\n",
      "Epoch 43/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5381 - mse: 0.5327 - mae: 0.5145 - val_loss: 0.4482 - val_mse: 0.4428 - val_mae: 0.4460\n",
      "Epoch 44/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5328 - mse: 0.5273 - mae: 0.5142 - val_loss: 0.4455 - val_mse: 0.4400 - val_mae: 0.4474\n",
      "Epoch 45/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5263 - mse: 0.5208 - mae: 0.5088 - val_loss: 0.4430 - val_mse: 0.4375 - val_mae: 0.4436\n",
      "Epoch 46/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5214 - mse: 0.5159 - mae: 0.5061 - val_loss: 0.4389 - val_mse: 0.4334 - val_mae: 0.4438\n",
      "Epoch 47/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5167 - mse: 0.5111 - mae: 0.5066 - val_loss: 0.4397 - val_mse: 0.4341 - val_mae: 0.4459\n",
      "Epoch 48/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5112 - mse: 0.5056 - mae: 0.5049 - val_loss: 0.4322 - val_mse: 0.4266 - val_mae: 0.4404\n",
      "Epoch 49/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5066 - mse: 0.5010 - mae: 0.5020 - val_loss: 0.4332 - val_mse: 0.4276 - val_mae: 0.4410\n",
      "Epoch 50/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5007 - mse: 0.4951 - mae: 0.4981 - val_loss: 0.4284 - val_mse: 0.4228 - val_mae: 0.4400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4963 - mse: 0.4907 - mae: 0.4951 - val_loss: 0.4244 - val_mse: 0.4187 - val_mae: 0.4360\n",
      "Epoch 52/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4914 - mse: 0.4858 - mae: 0.4925 - val_loss: 0.4262 - val_mse: 0.4205 - val_mae: 0.4395\n",
      "Epoch 53/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4884 - mse: 0.4827 - mae: 0.4942 - val_loss: 0.4227 - val_mse: 0.4170 - val_mae: 0.4421\n",
      "Epoch 54/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4831 - mse: 0.4774 - mae: 0.4903 - val_loss: 0.4155 - val_mse: 0.4098 - val_mae: 0.4306\n",
      "Epoch 55/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4772 - mse: 0.4715 - mae: 0.4862 - val_loss: 0.4210 - val_mse: 0.4152 - val_mae: 0.4401\n",
      "Epoch 56/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4740 - mse: 0.4682 - mae: 0.4859 - val_loss: 0.4136 - val_mse: 0.4078 - val_mae: 0.4359\n",
      "Epoch 57/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4701 - mse: 0.4644 - mae: 0.4839 - val_loss: 0.4105 - val_mse: 0.4047 - val_mae: 0.4342\n",
      "Epoch 58/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4662 - mse: 0.4604 - mae: 0.4802 - val_loss: 0.4110 - val_mse: 0.4052 - val_mae: 0.4320\n",
      "Epoch 59/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4612 - mse: 0.4553 - mae: 0.4770 - val_loss: 0.4094 - val_mse: 0.4036 - val_mae: 0.4338\n",
      "Epoch 60/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4568 - mse: 0.4510 - mae: 0.4770 - val_loss: 0.4007 - val_mse: 0.3949 - val_mae: 0.4276\n",
      "Epoch 61/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4524 - mse: 0.4465 - mae: 0.4735 - val_loss: 0.3986 - val_mse: 0.3927 - val_mae: 0.4262\n",
      "Epoch 62/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4492 - mse: 0.4433 - mae: 0.4725 - val_loss: 0.4032 - val_mse: 0.3973 - val_mae: 0.4316\n",
      "Epoch 63/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4452 - mse: 0.4393 - mae: 0.4695 - val_loss: 0.3930 - val_mse: 0.3871 - val_mae: 0.4242\n",
      "Epoch 64/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4401 - mse: 0.4342 - mae: 0.4668 - val_loss: 0.3947 - val_mse: 0.3887 - val_mae: 0.4272\n",
      "Epoch 65/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4382 - mse: 0.4322 - mae: 0.4656 - val_loss: 0.3940 - val_mse: 0.3880 - val_mae: 0.4242\n",
      "Epoch 66/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4348 - mse: 0.4288 - mae: 0.4642 - val_loss: 0.3870 - val_mse: 0.3810 - val_mae: 0.4219\n",
      "Epoch 67/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4306 - mse: 0.4246 - mae: 0.4597 - val_loss: 0.3902 - val_mse: 0.3841 - val_mae: 0.4213\n",
      "Epoch 68/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4247 - mse: 0.4187 - mae: 0.4560 - val_loss: 0.3875 - val_mse: 0.3814 - val_mae: 0.4238\n",
      "Epoch 69/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4241 - mse: 0.4180 - mae: 0.4574 - val_loss: 0.3793 - val_mse: 0.3732 - val_mae: 0.4151\n",
      "Epoch 70/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4185 - mse: 0.4125 - mae: 0.4526 - val_loss: 0.3810 - val_mse: 0.3749 - val_mae: 0.4208\n",
      "Epoch 71/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4154 - mse: 0.4093 - mae: 0.4513 - val_loss: 0.3800 - val_mse: 0.3739 - val_mae: 0.4215\n",
      "Epoch 72/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4126 - mse: 0.4065 - mae: 0.4501 - val_loss: 0.3784 - val_mse: 0.3723 - val_mae: 0.4184\n",
      "Epoch 73/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4093 - mse: 0.4032 - mae: 0.4476 - val_loss: 0.3742 - val_mse: 0.3680 - val_mae: 0.4199\n",
      "Epoch 74/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4072 - mse: 0.4011 - mae: 0.4484 - val_loss: 0.3746 - val_mse: 0.3685 - val_mae: 0.4185\n",
      "Epoch 75/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4032 - mse: 0.3971 - mae: 0.4430 - val_loss: 0.3704 - val_mse: 0.3642 - val_mae: 0.4132\n",
      "Epoch 76/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4025 - mse: 0.3963 - mae: 0.4440 - val_loss: 0.3681 - val_mse: 0.3619 - val_mae: 0.4185\n",
      "Epoch 77/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3978 - mse: 0.3916 - mae: 0.4440 - val_loss: 0.3691 - val_mse: 0.3629 - val_mae: 0.4181\n",
      "Epoch 78/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3941 - mse: 0.3879 - mae: 0.4396 - val_loss: 0.3643 - val_mse: 0.3580 - val_mae: 0.4092\n",
      "Epoch 79/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3927 - mse: 0.3865 - mae: 0.4372 - val_loss: 0.3660 - val_mse: 0.3597 - val_mae: 0.4158\n",
      "Epoch 80/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3905 - mse: 0.3843 - mae: 0.4372 - val_loss: 0.3626 - val_mse: 0.3564 - val_mae: 0.4147\n",
      "Epoch 81/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3877 - mse: 0.3814 - mae: 0.4350 - val_loss: 0.3572 - val_mse: 0.3509 - val_mae: 0.4093\n",
      "Epoch 82/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3822 - mse: 0.3759 - mae: 0.4332 - val_loss: 0.3622 - val_mse: 0.3559 - val_mae: 0.4192\n",
      "Epoch 83/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3799 - mse: 0.3735 - mae: 0.4315 - val_loss: 0.3566 - val_mse: 0.3503 - val_mae: 0.4103\n",
      "Epoch 84/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3779 - mse: 0.3715 - mae: 0.4281 - val_loss: 0.3505 - val_mse: 0.3441 - val_mae: 0.4075\n",
      "Epoch 85/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3767 - mse: 0.3704 - mae: 0.4310 - val_loss: 0.3536 - val_mse: 0.3472 - val_mae: 0.4112\n",
      "Epoch 86/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3724 - mse: 0.3660 - mae: 0.4254 - val_loss: 0.3453 - val_mse: 0.3389 - val_mae: 0.4035\n",
      "Epoch 87/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3705 - mse: 0.3641 - mae: 0.4264 - val_loss: 0.3507 - val_mse: 0.3443 - val_mae: 0.4090\n",
      "Epoch 88/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3661 - mse: 0.3596 - mae: 0.4215 - val_loss: 0.3448 - val_mse: 0.3383 - val_mae: 0.4068\n",
      "Epoch 89/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3638 - mse: 0.3573 - mae: 0.4212 - val_loss: 0.3438 - val_mse: 0.3374 - val_mae: 0.4071\n",
      "Epoch 90/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3610 - mse: 0.3546 - mae: 0.4206 - val_loss: 0.3429 - val_mse: 0.3364 - val_mae: 0.4054\n",
      "Epoch 91/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3591 - mse: 0.3526 - mae: 0.4192 - val_loss: 0.3383 - val_mse: 0.3318 - val_mae: 0.4059\n",
      "Epoch 92/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3573 - mse: 0.3508 - mae: 0.4156 - val_loss: 0.3382 - val_mse: 0.3316 - val_mae: 0.4030\n",
      "Epoch 93/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3531 - mse: 0.3466 - mae: 0.4165 - val_loss: 0.3365 - val_mse: 0.3299 - val_mae: 0.4051\n",
      "Epoch 94/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3511 - mse: 0.3446 - mae: 0.4138 - val_loss: 0.3352 - val_mse: 0.3286 - val_mae: 0.4052\n",
      "Epoch 95/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3496 - mse: 0.3430 - mae: 0.4122 - val_loss: 0.3293 - val_mse: 0.3227 - val_mae: 0.3988\n",
      "Epoch 96/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3448 - mse: 0.3382 - mae: 0.4112 - val_loss: 0.3318 - val_mse: 0.3252 - val_mae: 0.4016\n",
      "Epoch 97/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3431 - mse: 0.3365 - mae: 0.4087 - val_loss: 0.3266 - val_mse: 0.3200 - val_mae: 0.3950\n",
      "Epoch 98/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3409 - mse: 0.3343 - mae: 0.4090 - val_loss: 0.3219 - val_mse: 0.3153 - val_mae: 0.3902\n",
      "Epoch 99/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3380 - mse: 0.3314 - mae: 0.4049 - val_loss: 0.3207 - val_mse: 0.3141 - val_mae: 0.3941\n",
      "Epoch 100/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3346 - mse: 0.3279 - mae: 0.4056 - val_loss: 0.3211 - val_mse: 0.3144 - val_mae: 0.3952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3328 - mse: 0.3261 - mae: 0.4033 - val_loss: 0.3178 - val_mse: 0.3111 - val_mae: 0.3901\n",
      "Epoch 102/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3294 - mse: 0.3227 - mae: 0.4000 - val_loss: 0.3144 - val_mse: 0.3077 - val_mae: 0.3861\n",
      "Epoch 103/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3282 - mse: 0.3214 - mae: 0.3999 - val_loss: 0.3182 - val_mse: 0.3115 - val_mae: 0.3963\n",
      "Epoch 104/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3259 - mse: 0.3191 - mae: 0.3998 - val_loss: 0.3114 - val_mse: 0.3046 - val_mae: 0.3873\n",
      "Epoch 105/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3225 - mse: 0.3157 - mae: 0.3979 - val_loss: 0.3091 - val_mse: 0.3023 - val_mae: 0.3884\n",
      "Epoch 106/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3209 - mse: 0.3141 - mae: 0.3973 - val_loss: 0.3098 - val_mse: 0.3030 - val_mae: 0.3874\n",
      "Epoch 107/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3171 - mse: 0.3103 - mae: 0.3943 - val_loss: 0.3041 - val_mse: 0.2973 - val_mae: 0.3843\n",
      "Epoch 108/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3152 - mse: 0.3084 - mae: 0.3934 - val_loss: 0.3047 - val_mse: 0.2978 - val_mae: 0.3851\n",
      "Epoch 109/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3146 - mse: 0.3078 - mae: 0.3927 - val_loss: 0.3034 - val_mse: 0.2965 - val_mae: 0.3808\n",
      "Epoch 110/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3120 - mse: 0.3051 - mae: 0.3919 - val_loss: 0.3003 - val_mse: 0.2934 - val_mae: 0.3874\n",
      "Epoch 111/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3095 - mse: 0.3026 - mae: 0.3905 - val_loss: 0.3008 - val_mse: 0.2939 - val_mae: 0.3850\n",
      "Epoch 112/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3085 - mse: 0.3016 - mae: 0.3894 - val_loss: 0.3005 - val_mse: 0.2935 - val_mae: 0.3798\n",
      "Epoch 113/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3077 - mse: 0.3008 - mae: 0.3881 - val_loss: 0.2971 - val_mse: 0.2901 - val_mae: 0.3874\n",
      "Epoch 114/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3043 - mse: 0.2973 - mae: 0.3909 - val_loss: 0.2939 - val_mse: 0.2869 - val_mae: 0.3793\n",
      "Epoch 115/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3032 - mse: 0.2962 - mae: 0.3875 - val_loss: 0.2918 - val_mse: 0.2848 - val_mae: 0.3799\n",
      "Epoch 116/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3004 - mse: 0.2934 - mae: 0.3879 - val_loss: 0.2944 - val_mse: 0.2874 - val_mae: 0.3768\n",
      "Epoch 117/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2969 - mse: 0.2899 - mae: 0.3815 - val_loss: 0.2882 - val_mse: 0.2811 - val_mae: 0.3764\n",
      "Epoch 118/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2949 - mse: 0.2879 - mae: 0.3828 - val_loss: 0.2874 - val_mse: 0.2803 - val_mae: 0.3780\n",
      "Epoch 119/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2946 - mse: 0.2875 - mae: 0.3805 - val_loss: 0.2889 - val_mse: 0.2818 - val_mae: 0.3826\n",
      "Epoch 120/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2915 - mse: 0.2844 - mae: 0.3812 - val_loss: 0.2835 - val_mse: 0.2764 - val_mae: 0.3742\n",
      "Epoch 121/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2885 - mse: 0.2814 - mae: 0.3759 - val_loss: 0.2844 - val_mse: 0.2773 - val_mae: 0.3767\n",
      "Epoch 122/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2885 - mse: 0.2814 - mae: 0.3780 - val_loss: 0.2801 - val_mse: 0.2729 - val_mae: 0.3721\n",
      "Epoch 123/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2851 - mse: 0.2779 - mae: 0.3745 - val_loss: 0.2794 - val_mse: 0.2722 - val_mae: 0.3745\n",
      "Epoch 124/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2857 - mse: 0.2785 - mae: 0.3756 - val_loss: 0.2815 - val_mse: 0.2744 - val_mae: 0.3711\n",
      "Epoch 125/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2820 - mse: 0.2748 - mae: 0.3738 - val_loss: 0.2744 - val_mse: 0.2672 - val_mae: 0.3709\n",
      "Epoch 126/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2792 - mse: 0.2720 - mae: 0.3705 - val_loss: 0.2767 - val_mse: 0.2695 - val_mae: 0.3679\n",
      "Epoch 127/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2786 - mse: 0.2714 - mae: 0.3707 - val_loss: 0.2730 - val_mse: 0.2657 - val_mae: 0.3707\n",
      "Epoch 128/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2765 - mse: 0.2693 - mae: 0.3677 - val_loss: 0.2718 - val_mse: 0.2645 - val_mae: 0.3622\n",
      "Epoch 129/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2747 - mse: 0.2674 - mae: 0.3686 - val_loss: 0.2708 - val_mse: 0.2635 - val_mae: 0.3686\n",
      "Epoch 130/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2732 - mse: 0.2659 - mae: 0.3664 - val_loss: 0.2685 - val_mse: 0.2612 - val_mae: 0.3606\n",
      "Epoch 131/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2707 - mse: 0.2634 - mae: 0.3644 - val_loss: 0.2680 - val_mse: 0.2607 - val_mae: 0.3678\n",
      "Epoch 132/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2692 - mse: 0.2619 - mae: 0.3652 - val_loss: 0.2621 - val_mse: 0.2548 - val_mae: 0.3604\n",
      "Epoch 133/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2675 - mse: 0.2602 - mae: 0.3640 - val_loss: 0.2637 - val_mse: 0.2563 - val_mae: 0.3589\n",
      "Epoch 134/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2664 - mse: 0.2591 - mae: 0.3611 - val_loss: 0.2645 - val_mse: 0.2571 - val_mae: 0.3624\n",
      "Epoch 135/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2629 - mse: 0.2555 - mae: 0.3614 - val_loss: 0.2621 - val_mse: 0.2547 - val_mae: 0.3587\n",
      "Epoch 136/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2629 - mse: 0.2555 - mae: 0.3582 - val_loss: 0.2577 - val_mse: 0.2503 - val_mae: 0.3581\n",
      "Epoch 137/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2588 - mse: 0.2514 - mae: 0.3556 - val_loss: 0.2565 - val_mse: 0.2490 - val_mae: 0.3571\n",
      "Epoch 138/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2577 - mse: 0.2502 - mae: 0.3547 - val_loss: 0.2571 - val_mse: 0.2497 - val_mae: 0.3564\n",
      "Epoch 139/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2548 - mse: 0.2474 - mae: 0.3522 - val_loss: 0.2539 - val_mse: 0.2464 - val_mae: 0.3548\n",
      "Epoch 140/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2531 - mse: 0.2456 - mae: 0.3525 - val_loss: 0.2543 - val_mse: 0.2468 - val_mae: 0.3559\n",
      "Epoch 141/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2530 - mse: 0.2455 - mae: 0.3502 - val_loss: 0.2522 - val_mse: 0.2447 - val_mae: 0.3529\n",
      "Epoch 142/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2493 - mse: 0.2418 - mae: 0.3489 - val_loss: 0.2512 - val_mse: 0.2437 - val_mae: 0.3540\n",
      "Epoch 143/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2476 - mse: 0.2400 - mae: 0.3466 - val_loss: 0.2479 - val_mse: 0.2403 - val_mae: 0.3488\n",
      "Epoch 144/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2456 - mse: 0.2381 - mae: 0.3450 - val_loss: 0.2459 - val_mse: 0.2383 - val_mae: 0.3507\n",
      "Epoch 145/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2452 - mse: 0.2377 - mae: 0.3434 - val_loss: 0.2442 - val_mse: 0.2366 - val_mae: 0.3464\n",
      "Epoch 146/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2463 - mse: 0.2387 - mae: 0.3550 - val_loss: 0.2441 - val_mse: 0.2365 - val_mae: 0.3457\n",
      "Epoch 147/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2440 - mse: 0.2364 - mae: 0.3443 - val_loss: 0.2413 - val_mse: 0.2337 - val_mae: 0.3494\n",
      "Epoch 148/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2394 - mse: 0.2318 - mae: 0.3414 - val_loss: 0.2402 - val_mse: 0.2326 - val_mae: 0.3473\n",
      "Epoch 149/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2374 - mse: 0.2297 - mae: 0.3385 - val_loss: 0.2397 - val_mse: 0.2321 - val_mae: 0.3443\n",
      "Epoch 150/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2363 - mse: 0.2286 - mae: 0.3378 - val_loss: 0.2379 - val_mse: 0.2303 - val_mae: 0.3430\n",
      "Epoch 151/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2342 - mse: 0.2265 - mae: 0.3368 - val_loss: 0.2371 - val_mse: 0.2294 - val_mae: 0.3461\n",
      "Epoch 152/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2333 - mse: 0.2256 - mae: 0.3359 - val_loss: 0.2340 - val_mse: 0.2263 - val_mae: 0.3382\n",
      "Epoch 153/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2322 - mse: 0.2245 - mae: 0.3365 - val_loss: 0.2332 - val_mse: 0.2254 - val_mae: 0.3431\n",
      "Epoch 154/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2301 - mse: 0.2224 - mae: 0.3338 - val_loss: 0.2332 - val_mse: 0.2254 - val_mae: 0.3402\n",
      "Epoch 155/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2307 - mse: 0.2229 - mae: 0.3368 - val_loss: 0.2311 - val_mse: 0.2233 - val_mae: 0.3385\n",
      "Epoch 156/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2287 - mse: 0.2209 - mae: 0.3332 - val_loss: 0.2273 - val_mse: 0.2195 - val_mae: 0.3400\n",
      "Epoch 157/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2246 - mse: 0.2168 - mae: 0.3296 - val_loss: 0.2297 - val_mse: 0.2219 - val_mae: 0.3359\n",
      "Epoch 158/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2238 - mse: 0.2160 - mae: 0.3291 - val_loss: 0.2268 - val_mse: 0.2189 - val_mae: 0.3368\n",
      "Epoch 159/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2232 - mse: 0.2153 - mae: 0.3280 - val_loss: 0.2262 - val_mse: 0.2184 - val_mae: 0.3413\n",
      "Epoch 160/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2205 - mse: 0.2127 - mae: 0.3266 - val_loss: 0.2249 - val_mse: 0.2171 - val_mae: 0.3347\n",
      "Epoch 161/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2190 - mse: 0.2112 - mae: 0.3240 - val_loss: 0.2234 - val_mse: 0.2155 - val_mae: 0.3359\n",
      "Epoch 162/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2181 - mse: 0.2102 - mae: 0.3267 - val_loss: 0.2194 - val_mse: 0.2115 - val_mae: 0.3328\n",
      "Epoch 163/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2175 - mse: 0.2096 - mae: 0.3241 - val_loss: 0.2236 - val_mse: 0.2157 - val_mae: 0.3431\n",
      "Epoch 164/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2154 - mse: 0.2075 - mae: 0.3256 - val_loss: 0.2195 - val_mse: 0.2115 - val_mae: 0.3306\n",
      "Epoch 165/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2153 - mse: 0.2073 - mae: 0.3223 - val_loss: 0.2195 - val_mse: 0.2115 - val_mae: 0.3401\n",
      "Epoch 166/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2127 - mse: 0.2047 - mae: 0.3220 - val_loss: 0.2166 - val_mse: 0.2086 - val_mae: 0.3305\n",
      "Epoch 167/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2110 - mse: 0.2030 - mae: 0.3162 - val_loss: 0.2181 - val_mse: 0.2101 - val_mae: 0.3382\n",
      "Epoch 168/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2101 - mse: 0.2021 - mae: 0.3210 - val_loss: 0.2146 - val_mse: 0.2066 - val_mae: 0.3287\n",
      "Epoch 169/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2077 - mse: 0.1997 - mae: 0.3166 - val_loss: 0.2143 - val_mse: 0.2062 - val_mae: 0.3307\n",
      "Epoch 170/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2060 - mse: 0.1980 - mae: 0.3153 - val_loss: 0.2122 - val_mse: 0.2042 - val_mae: 0.3337\n",
      "Epoch 171/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2060 - mse: 0.1979 - mae: 0.3152 - val_loss: 0.2085 - val_mse: 0.2004 - val_mae: 0.3273\n",
      "Epoch 172/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2033 - mse: 0.1952 - mae: 0.3137 - val_loss: 0.2095 - val_mse: 0.2014 - val_mae: 0.3251\n",
      "Epoch 173/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2031 - mse: 0.1950 - mae: 0.3113 - val_loss: 0.2115 - val_mse: 0.2034 - val_mae: 0.3324\n",
      "Epoch 174/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2015 - mse: 0.1934 - mae: 0.3137 - val_loss: 0.2052 - val_mse: 0.1970 - val_mae: 0.3266\n",
      "Epoch 175/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2007 - mse: 0.1926 - mae: 0.3104 - val_loss: 0.2103 - val_mse: 0.2022 - val_mae: 0.3342\n",
      "Epoch 176/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2014 - mse: 0.1932 - mae: 0.3125 - val_loss: 0.2063 - val_mse: 0.1981 - val_mae: 0.3306\n",
      "Epoch 177/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1992 - mse: 0.1910 - mae: 0.3127 - val_loss: 0.2030 - val_mse: 0.1948 - val_mae: 0.3218\n",
      "Epoch 178/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1961 - mse: 0.1879 - mae: 0.3072 - val_loss: 0.2045 - val_mse: 0.1963 - val_mae: 0.3266\n",
      "Epoch 179/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1955 - mse: 0.1872 - mae: 0.3068 - val_loss: 0.2002 - val_mse: 0.1920 - val_mae: 0.3234\n",
      "Epoch 180/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1942 - mse: 0.1859 - mae: 0.3084 - val_loss: 0.2005 - val_mse: 0.1923 - val_mae: 0.3242\n",
      "Epoch 181/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1934 - mse: 0.1851 - mae: 0.3057 - val_loss: 0.1990 - val_mse: 0.1907 - val_mae: 0.3234\n",
      "Epoch 182/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1906 - mse: 0.1823 - mae: 0.3045 - val_loss: 0.1987 - val_mse: 0.1904 - val_mae: 0.3230\n",
      "Epoch 183/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1898 - mse: 0.1815 - mae: 0.3042 - val_loss: 0.1981 - val_mse: 0.1898 - val_mae: 0.3224\n",
      "Epoch 184/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1883 - mse: 0.1799 - mae: 0.3016 - val_loss: 0.1976 - val_mse: 0.1893 - val_mae: 0.3228\n",
      "Epoch 185/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1868 - mse: 0.1785 - mae: 0.3018 - val_loss: 0.1972 - val_mse: 0.1888 - val_mae: 0.3214\n",
      "Epoch 186/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1869 - mse: 0.1785 - mae: 0.3021 - val_loss: 0.1947 - val_mse: 0.1863 - val_mae: 0.3168\n",
      "Epoch 187/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1856 - mse: 0.1772 - mae: 0.2993 - val_loss: 0.1938 - val_mse: 0.1854 - val_mae: 0.3189\n",
      "Epoch 188/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1836 - mse: 0.1752 - mae: 0.2998 - val_loss: 0.1934 - val_mse: 0.1850 - val_mae: 0.3221\n",
      "Epoch 189/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1831 - mse: 0.1747 - mae: 0.2995 - val_loss: 0.1929 - val_mse: 0.1845 - val_mae: 0.3195\n",
      "Epoch 190/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1827 - mse: 0.1743 - mae: 0.3010 - val_loss: 0.1923 - val_mse: 0.1838 - val_mae: 0.3186\n",
      "Epoch 191/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1831 - mse: 0.1746 - mae: 0.2983 - val_loss: 0.1953 - val_mse: 0.1869 - val_mae: 0.3219\n",
      "Epoch 192/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1800 - mse: 0.1715 - mae: 0.2963 - val_loss: 0.1881 - val_mse: 0.1797 - val_mae: 0.3182\n",
      "Epoch 193/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1778 - mse: 0.1693 - mae: 0.2937 - val_loss: 0.1884 - val_mse: 0.1799 - val_mae: 0.3156\n",
      "Epoch 194/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1770 - mse: 0.1685 - mae: 0.2941 - val_loss: 0.1901 - val_mse: 0.1815 - val_mae: 0.3220\n",
      "Epoch 195/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1755 - mse: 0.1670 - mae: 0.2939 - val_loss: 0.1878 - val_mse: 0.1793 - val_mae: 0.3163\n",
      "Epoch 196/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1743 - mse: 0.1657 - mae: 0.2913 - val_loss: 0.1882 - val_mse: 0.1797 - val_mae: 0.3195\n",
      "Epoch 197/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1735 - mse: 0.1649 - mae: 0.2917 - val_loss: 0.1843 - val_mse: 0.1757 - val_mae: 0.3109\n",
      "Epoch 198/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1709 - mse: 0.1623 - mae: 0.2885 - val_loss: 0.1846 - val_mse: 0.1760 - val_mae: 0.3149\n",
      "Epoch 199/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1694 - mse: 0.1608 - mae: 0.2875 - val_loss: 0.1825 - val_mse: 0.1739 - val_mae: 0.3131\n",
      "Epoch 200/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1685 - mse: 0.1599 - mae: 0.2877 - val_loss: 0.1801 - val_mse: 0.1715 - val_mae: 0.3118\n",
      "Epoch 201/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1690 - mse: 0.1604 - mae: 0.2894 - val_loss: 0.1790 - val_mse: 0.1704 - val_mae: 0.3124\n",
      "Epoch 202/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1671 - mse: 0.1585 - mae: 0.2856 - val_loss: 0.1800 - val_mse: 0.1713 - val_mae: 0.3068\n",
      "Epoch 203/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1646 - mse: 0.1559 - mae: 0.2830 - val_loss: 0.1766 - val_mse: 0.1680 - val_mae: 0.3109\n",
      "Epoch 204/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1638 - mse: 0.1551 - mae: 0.2830 - val_loss: 0.1770 - val_mse: 0.1683 - val_mae: 0.3105\n",
      "Epoch 205/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1630 - mse: 0.1542 - mae: 0.2820 - val_loss: 0.1749 - val_mse: 0.1662 - val_mae: 0.3099\n",
      "Epoch 206/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1633 - mse: 0.1545 - mae: 0.2845 - val_loss: 0.1779 - val_mse: 0.1691 - val_mae: 0.3126\n",
      "Epoch 207/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1615 - mse: 0.1527 - mae: 0.2828 - val_loss: 0.1762 - val_mse: 0.1674 - val_mae: 0.3081\n",
      "Epoch 208/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1599 - mse: 0.1511 - mae: 0.2810 - val_loss: 0.1714 - val_mse: 0.1626 - val_mae: 0.3049\n",
      "Epoch 209/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1580 - mse: 0.1492 - mae: 0.2777 - val_loss: 0.1713 - val_mse: 0.1625 - val_mae: 0.3057\n",
      "Epoch 210/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1577 - mse: 0.1488 - mae: 0.2783 - val_loss: 0.1720 - val_mse: 0.1632 - val_mae: 0.3077\n",
      "Epoch 211/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1554 - mse: 0.1466 - mae: 0.2767 - val_loss: 0.1702 - val_mse: 0.1614 - val_mae: 0.3049\n",
      "Epoch 212/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1552 - mse: 0.1463 - mae: 0.2750 - val_loss: 0.1679 - val_mse: 0.1591 - val_mae: 0.3025\n",
      "Epoch 213/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1538 - mse: 0.1449 - mae: 0.2749 - val_loss: 0.1677 - val_mse: 0.1588 - val_mae: 0.3021\n",
      "Epoch 214/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1534 - mse: 0.1445 - mae: 0.2754 - val_loss: 0.1680 - val_mse: 0.1590 - val_mae: 0.3044\n",
      "Epoch 215/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1521 - mse: 0.1431 - mae: 0.2748 - val_loss: 0.1665 - val_mse: 0.1576 - val_mae: 0.2995\n",
      "Epoch 216/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1515 - mse: 0.1426 - mae: 0.2722 - val_loss: 0.1670 - val_mse: 0.1580 - val_mae: 0.3026\n",
      "Epoch 217/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1519 - mse: 0.1430 - mae: 0.2748 - val_loss: 0.1627 - val_mse: 0.1537 - val_mae: 0.2995\n",
      "Epoch 218/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1499 - mse: 0.1409 - mae: 0.2730 - val_loss: 0.1646 - val_mse: 0.1556 - val_mae: 0.3020\n",
      "Epoch 219/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1507 - mse: 0.1417 - mae: 0.2740 - val_loss: 0.1616 - val_mse: 0.1526 - val_mae: 0.3000\n",
      "Epoch 220/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1483 - mse: 0.1393 - mae: 0.2705 - val_loss: 0.1617 - val_mse: 0.1527 - val_mae: 0.3008\n",
      "Epoch 221/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1496 - mse: 0.1406 - mae: 0.2744 - val_loss: 0.1594 - val_mse: 0.1504 - val_mae: 0.2951\n",
      "Epoch 222/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1465 - mse: 0.1374 - mae: 0.2682 - val_loss: 0.1611 - val_mse: 0.1520 - val_mae: 0.2960\n",
      "Epoch 223/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1453 - mse: 0.1362 - mae: 0.2671 - val_loss: 0.1586 - val_mse: 0.1496 - val_mae: 0.2959\n",
      "Epoch 224/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1444 - mse: 0.1353 - mae: 0.2683 - val_loss: 0.1576 - val_mse: 0.1485 - val_mae: 0.2961\n",
      "Epoch 225/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1457 - mse: 0.1366 - mae: 0.2678 - val_loss: 0.1575 - val_mse: 0.1484 - val_mae: 0.2960\n",
      "Epoch 226/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1436 - mse: 0.1345 - mae: 0.2690 - val_loss: 0.1579 - val_mse: 0.1488 - val_mae: 0.2952\n",
      "Epoch 227/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1415 - mse: 0.1324 - mae: 0.2639 - val_loss: 0.1552 - val_mse: 0.1461 - val_mae: 0.2930\n",
      "Epoch 228/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1413 - mse: 0.1321 - mae: 0.2645 - val_loss: 0.1552 - val_mse: 0.1460 - val_mae: 0.2933\n",
      "Epoch 229/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1407 - mse: 0.1316 - mae: 0.2639 - val_loss: 0.1558 - val_mse: 0.1466 - val_mae: 0.2924\n",
      "Epoch 230/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1411 - mse: 0.1319 - mae: 0.2661 - val_loss: 0.1549 - val_mse: 0.1458 - val_mae: 0.2968\n",
      "Epoch 231/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1393 - mse: 0.1301 - mae: 0.2642 - val_loss: 0.1529 - val_mse: 0.1437 - val_mae: 0.2882\n",
      "Epoch 232/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1396 - mse: 0.1304 - mae: 0.2658 - val_loss: 0.1519 - val_mse: 0.1427 - val_mae: 0.2925\n",
      "Epoch 233/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1368 - mse: 0.1276 - mae: 0.2608 - val_loss: 0.1511 - val_mse: 0.1419 - val_mae: 0.2875\n",
      "Epoch 234/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1379 - mse: 0.1287 - mae: 0.2646 - val_loss: 0.1513 - val_mse: 0.1420 - val_mae: 0.2913\n",
      "Epoch 235/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1356 - mse: 0.1264 - mae: 0.2605 - val_loss: 0.1503 - val_mse: 0.1411 - val_mae: 0.2877\n",
      "Epoch 236/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1368 - mse: 0.1276 - mae: 0.2612 - val_loss: 0.1505 - val_mse: 0.1412 - val_mae: 0.2902\n",
      "Epoch 237/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1347 - mse: 0.1255 - mae: 0.2601 - val_loss: 0.1492 - val_mse: 0.1399 - val_mae: 0.2876\n",
      "Epoch 238/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1345 - mse: 0.1252 - mae: 0.2610 - val_loss: 0.1483 - val_mse: 0.1390 - val_mae: 0.2849\n",
      "Epoch 239/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1340 - mse: 0.1247 - mae: 0.2601 - val_loss: 0.1489 - val_mse: 0.1396 - val_mae: 0.2895\n",
      "Epoch 240/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1324 - mse: 0.1230 - mae: 0.2567 - val_loss: 0.1480 - val_mse: 0.1387 - val_mae: 0.2847\n",
      "Epoch 241/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1319 - mse: 0.1225 - mae: 0.2583 - val_loss: 0.1455 - val_mse: 0.1362 - val_mae: 0.2846\n",
      "Epoch 242/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1323 - mse: 0.1230 - mae: 0.2589 - val_loss: 0.1451 - val_mse: 0.1357 - val_mae: 0.2856\n",
      "Epoch 243/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1317 - mse: 0.1223 - mae: 0.2584 - val_loss: 0.1477 - val_mse: 0.1383 - val_mae: 0.2881\n",
      "Epoch 244/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1328 - mse: 0.1234 - mae: 0.2631 - val_loss: 0.1470 - val_mse: 0.1376 - val_mae: 0.2853\n",
      "Epoch 245/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1293 - mse: 0.1199 - mae: 0.2548 - val_loss: 0.1447 - val_mse: 0.1353 - val_mae: 0.2858\n",
      "Epoch 246/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1295 - mse: 0.1201 - mae: 0.2558 - val_loss: 0.1432 - val_mse: 0.1337 - val_mae: 0.2817\n",
      "Epoch 247/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1307 - mse: 0.1213 - mae: 0.2592 - val_loss: 0.1419 - val_mse: 0.1324 - val_mae: 0.2817\n",
      "Epoch 248/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1290 - mse: 0.1196 - mae: 0.2547 - val_loss: 0.1426 - val_mse: 0.1332 - val_mae: 0.2828\n",
      "Epoch 249/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1272 - mse: 0.1178 - mae: 0.2539 - val_loss: 0.1413 - val_mse: 0.1318 - val_mae: 0.2826\n",
      "Epoch 250/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1263 - mse: 0.1169 - mae: 0.2528 - val_loss: 0.1417 - val_mse: 0.1323 - val_mae: 0.2816\n",
      "Epoch 251/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1259 - mse: 0.1164 - mae: 0.2540 - val_loss: 0.1386 - val_mse: 0.1292 - val_mae: 0.2791\n",
      "Epoch 252/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1262 - mse: 0.1167 - mae: 0.2541 - val_loss: 0.1396 - val_mse: 0.1301 - val_mae: 0.2818\n",
      "Epoch 253/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1253 - mse: 0.1158 - mae: 0.2523 - val_loss: 0.1408 - val_mse: 0.1313 - val_mae: 0.2777\n",
      "Epoch 254/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1260 - mse: 0.1164 - mae: 0.2538 - val_loss: 0.1385 - val_mse: 0.1290 - val_mae: 0.2818\n",
      "Epoch 255/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1245 - mse: 0.1149 - mae: 0.2519 - val_loss: 0.1369 - val_mse: 0.1273 - val_mae: 0.2750\n",
      "Epoch 256/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1241 - mse: 0.1145 - mae: 0.2521 - val_loss: 0.1378 - val_mse: 0.1282 - val_mae: 0.2813\n",
      "Epoch 257/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1251 - mse: 0.1155 - mae: 0.2542 - val_loss: 0.1387 - val_mse: 0.1292 - val_mae: 0.2749\n",
      "Epoch 258/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1227 - mse: 0.1131 - mae: 0.2520 - val_loss: 0.1345 - val_mse: 0.1249 - val_mae: 0.2741\n",
      "Epoch 259/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1218 - mse: 0.1122 - mae: 0.2511 - val_loss: 0.1362 - val_mse: 0.1266 - val_mae: 0.2749\n",
      "Epoch 260/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1226 - mse: 0.1130 - mae: 0.2513 - val_loss: 0.1351 - val_mse: 0.1255 - val_mae: 0.2788\n",
      "Epoch 261/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1205 - mse: 0.1108 - mae: 0.2490 - val_loss: 0.1325 - val_mse: 0.1229 - val_mae: 0.2714\n",
      "Epoch 262/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1208 - mse: 0.1112 - mae: 0.2491 - val_loss: 0.1311 - val_mse: 0.1214 - val_mae: 0.2702\n",
      "Epoch 263/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1202 - mse: 0.1106 - mae: 0.2474 - val_loss: 0.1303 - val_mse: 0.1206 - val_mae: 0.2716\n",
      "Epoch 264/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1184 - mse: 0.1087 - mae: 0.2466 - val_loss: 0.1287 - val_mse: 0.1190 - val_mae: 0.2654\n",
      "Epoch 265/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1167 - mse: 0.1071 - mae: 0.2437 - val_loss: 0.1280 - val_mse: 0.1183 - val_mae: 0.2654\n",
      "Epoch 266/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1166 - mse: 0.1070 - mae: 0.2433 - val_loss: 0.1280 - val_mse: 0.1183 - val_mae: 0.2656\n",
      "Epoch 267/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1158 - mse: 0.1062 - mae: 0.2422 - val_loss: 0.1271 - val_mse: 0.1174 - val_mae: 0.2616\n",
      "Epoch 268/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1151 - mse: 0.1054 - mae: 0.2413 - val_loss: 0.1290 - val_mse: 0.1193 - val_mae: 0.2698\n",
      "Epoch 269/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1156 - mse: 0.1059 - mae: 0.2435 - val_loss: 0.1268 - val_mse: 0.1171 - val_mae: 0.2581\n",
      "Epoch 270/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1133 - mse: 0.1036 - mae: 0.2401 - val_loss: 0.1271 - val_mse: 0.1174 - val_mae: 0.2630\n",
      "Epoch 271/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1147 - mse: 0.1050 - mae: 0.2416 - val_loss: 0.1294 - val_mse: 0.1197 - val_mae: 0.2610\n",
      "Epoch 272/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1129 - mse: 0.1032 - mae: 0.2388 - val_loss: 0.1262 - val_mse: 0.1165 - val_mae: 0.2637\n",
      "Epoch 273/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1113 - mse: 0.1016 - mae: 0.2365 - val_loss: 0.1286 - val_mse: 0.1188 - val_mae: 0.2601\n",
      "Epoch 274/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1118 - mse: 0.1020 - mae: 0.2382 - val_loss: 0.1246 - val_mse: 0.1149 - val_mae: 0.2625\n",
      "Epoch 275/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1110 - mse: 0.1012 - mae: 0.2376 - val_loss: 0.1263 - val_mse: 0.1165 - val_mae: 0.2607\n",
      "Epoch 276/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1111 - mse: 0.1014 - mae: 0.2374 - val_loss: 0.1244 - val_mse: 0.1147 - val_mae: 0.2596\n",
      "Epoch 277/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1108 - mse: 0.1010 - mae: 0.2380 - val_loss: 0.1256 - val_mse: 0.1158 - val_mae: 0.2659\n",
      "Epoch 278/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1127 - mse: 0.1029 - mae: 0.2423 - val_loss: 0.1248 - val_mse: 0.1150 - val_mae: 0.2587\n",
      "Epoch 279/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1100 - mse: 0.1002 - mae: 0.2372 - val_loss: 0.1254 - val_mse: 0.1156 - val_mae: 0.2604\n",
      "Epoch 280/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1096 - mse: 0.0998 - mae: 0.2361 - val_loss: 0.1231 - val_mse: 0.1133 - val_mae: 0.2593\n",
      "Epoch 281/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1087 - mse: 0.0989 - mae: 0.2351 - val_loss: 0.1218 - val_mse: 0.1119 - val_mae: 0.2581\n",
      "Epoch 282/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1085 - mse: 0.0987 - mae: 0.2353 - val_loss: 0.1244 - val_mse: 0.1146 - val_mae: 0.2622\n",
      "Epoch 283/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1089 - mse: 0.0990 - mae: 0.2372 - val_loss: 0.1250 - val_mse: 0.1152 - val_mae: 0.2563\n",
      "Epoch 284/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1086 - mse: 0.0988 - mae: 0.2360 - val_loss: 0.1248 - val_mse: 0.1149 - val_mae: 0.2598\n",
      "Epoch 285/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1066 - mse: 0.0967 - mae: 0.2346 - val_loss: 0.1215 - val_mse: 0.1116 - val_mae: 0.2579\n",
      "Epoch 286/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1064 - mse: 0.0965 - mae: 0.2343 - val_loss: 0.1225 - val_mse: 0.1127 - val_mae: 0.2625\n",
      "Epoch 287/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1081 - mse: 0.0982 - mae: 0.2337 - val_loss: 0.1214 - val_mse: 0.1115 - val_mae: 0.2539\n",
      "Epoch 288/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1073 - mse: 0.0974 - mae: 0.2349 - val_loss: 0.1237 - val_mse: 0.1139 - val_mae: 0.2563\n",
      "Epoch 289/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1082 - mse: 0.0983 - mae: 0.2363 - val_loss: 0.1282 - val_mse: 0.1183 - val_mae: 0.2575\n",
      "Epoch 290/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1063 - mse: 0.0964 - mae: 0.2345 - val_loss: 0.1201 - val_mse: 0.1102 - val_mae: 0.2587\n",
      "Epoch 291/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1045 - mse: 0.0946 - mae: 0.2322 - val_loss: 0.1207 - val_mse: 0.1107 - val_mae: 0.2554\n",
      "Epoch 292/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1050 - mse: 0.0951 - mae: 0.2321 - val_loss: 0.1197 - val_mse: 0.1098 - val_mae: 0.2560\n",
      "Epoch 293/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1033 - mse: 0.0934 - mae: 0.2303 - val_loss: 0.1188 - val_mse: 0.1089 - val_mae: 0.2555\n",
      "Epoch 294/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1038 - mse: 0.0939 - mae: 0.2307 - val_loss: 0.1191 - val_mse: 0.1091 - val_mae: 0.2504\n",
      "Epoch 295/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1038 - mse: 0.0939 - mae: 0.2314 - val_loss: 0.1219 - val_mse: 0.1119 - val_mae: 0.2529\n",
      "Epoch 296/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1017 - mse: 0.0917 - mae: 0.2284 - val_loss: 0.1180 - val_mse: 0.1080 - val_mae: 0.2553\n",
      "Epoch 297/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1020 - mse: 0.0920 - mae: 0.2288 - val_loss: 0.1175 - val_mse: 0.1076 - val_mae: 0.2536\n",
      "Epoch 298/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1017 - mse: 0.0918 - mae: 0.2286 - val_loss: 0.1188 - val_mse: 0.1088 - val_mae: 0.2540\n",
      "Epoch 299/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1001 - mse: 0.0902 - mae: 0.2277 - val_loss: 0.1172 - val_mse: 0.1072 - val_mae: 0.2515\n",
      "Epoch 300/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0998 - mse: 0.0899 - mae: 0.2264 - val_loss: 0.1195 - val_mse: 0.1095 - val_mae: 0.2508\n",
      "Epoch 301/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0999 - mse: 0.0899 - mae: 0.2275 - val_loss: 0.1172 - val_mse: 0.1072 - val_mae: 0.2576\n",
      "Epoch 302/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0998 - mse: 0.0898 - mae: 0.2280 - val_loss: 0.1169 - val_mse: 0.1069 - val_mae: 0.2525\n",
      "Epoch 303/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0988 - mse: 0.0888 - mae: 0.2265 - val_loss: 0.1202 - val_mse: 0.1102 - val_mae: 0.2508\n",
      "Epoch 304/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0994 - mse: 0.0893 - mae: 0.2264 - val_loss: 0.1165 - val_mse: 0.1065 - val_mae: 0.2558\n",
      "Epoch 305/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0989 - mse: 0.0889 - mae: 0.2279 - val_loss: 0.1167 - val_mse: 0.1067 - val_mae: 0.2529\n",
      "Epoch 306/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0972 - mse: 0.0871 - mae: 0.2263 - val_loss: 0.1173 - val_mse: 0.1072 - val_mae: 0.2510\n",
      "Epoch 307/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0966 - mse: 0.0866 - mae: 0.2225 - val_loss: 0.1173 - val_mse: 0.1072 - val_mae: 0.2559\n",
      "Epoch 308/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0964 - mse: 0.0863 - mae: 0.2234 - val_loss: 0.1150 - val_mse: 0.1050 - val_mae: 0.2532\n",
      "Epoch 309/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0965 - mse: 0.0864 - mae: 0.2234 - val_loss: 0.1171 - val_mse: 0.1071 - val_mae: 0.2530\n",
      "Epoch 310/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0955 - mse: 0.0854 - mae: 0.2227 - val_loss: 0.1153 - val_mse: 0.1052 - val_mae: 0.2497\n",
      "Epoch 311/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0963 - mse: 0.0862 - mae: 0.2238 - val_loss: 0.1199 - val_mse: 0.1099 - val_mae: 0.2553\n",
      "Epoch 312/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0939 - mse: 0.0838 - mae: 0.2195 - val_loss: 0.1145 - val_mse: 0.1044 - val_mae: 0.2496\n",
      "Epoch 313/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0942 - mse: 0.0841 - mae: 0.2214 - val_loss: 0.1160 - val_mse: 0.1059 - val_mae: 0.2492\n",
      "Epoch 314/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0938 - mse: 0.0837 - mae: 0.2211 - val_loss: 0.1156 - val_mse: 0.1055 - val_mae: 0.2501\n",
      "Epoch 315/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0930 - mse: 0.0829 - mae: 0.2207 - val_loss: 0.1159 - val_mse: 0.1058 - val_mae: 0.2494\n",
      "Epoch 316/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0928 - mse: 0.0827 - mae: 0.2197 - val_loss: 0.1161 - val_mse: 0.1060 - val_mae: 0.2537\n",
      "Epoch 317/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0932 - mse: 0.0831 - mae: 0.2209 - val_loss: 0.1153 - val_mse: 0.1052 - val_mae: 0.2521\n",
      "Epoch 318/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0920 - mse: 0.0819 - mae: 0.2193 - val_loss: 0.1140 - val_mse: 0.1039 - val_mae: 0.2462\n",
      "Epoch 319/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0918 - mse: 0.0817 - mae: 0.2172 - val_loss: 0.1153 - val_mse: 0.1052 - val_mae: 0.2527\n",
      "Epoch 320/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0923 - mse: 0.0822 - mae: 0.2199 - val_loss: 0.1141 - val_mse: 0.1040 - val_mae: 0.2484\n",
      "Epoch 321/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0905 - mse: 0.0804 - mae: 0.2171 - val_loss: 0.1151 - val_mse: 0.1049 - val_mae: 0.2508\n",
      "Epoch 322/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0903 - mse: 0.0801 - mae: 0.2170 - val_loss: 0.1135 - val_mse: 0.1033 - val_mae: 0.2493\n",
      "Epoch 323/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0901 - mse: 0.0799 - mae: 0.2174 - val_loss: 0.1148 - val_mse: 0.1046 - val_mae: 0.2484\n",
      "Epoch 324/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0894 - mse: 0.0793 - mae: 0.2151 - val_loss: 0.1145 - val_mse: 0.1043 - val_mae: 0.2514\n",
      "Epoch 325/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0893 - mse: 0.0791 - mae: 0.2162 - val_loss: 0.1138 - val_mse: 0.1036 - val_mae: 0.2512\n",
      "Epoch 326/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0893 - mse: 0.0792 - mae: 0.2156 - val_loss: 0.1140 - val_mse: 0.1038 - val_mae: 0.2506\n",
      "Epoch 327/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0892 - mse: 0.0790 - mae: 0.2163 - val_loss: 0.1139 - val_mse: 0.1037 - val_mae: 0.2499\n",
      "Epoch 328/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0886 - mse: 0.0784 - mae: 0.2155 - val_loss: 0.1167 - val_mse: 0.1065 - val_mae: 0.2512\n",
      "Epoch 329/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0889 - mse: 0.0787 - mae: 0.2150 - val_loss: 0.1158 - val_mse: 0.1056 - val_mae: 0.2579\n",
      "Epoch 330/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0896 - mse: 0.0794 - mae: 0.2170 - val_loss: 0.1154 - val_mse: 0.1052 - val_mae: 0.2480\n",
      "Epoch 331/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0870 - mse: 0.0768 - mae: 0.2137 - val_loss: 0.1144 - val_mse: 0.1042 - val_mae: 0.2466\n",
      "Epoch 332/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0864 - mse: 0.0762 - mae: 0.2123 - val_loss: 0.1149 - val_mse: 0.1047 - val_mae: 0.2526\n",
      "Epoch 333/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0869 - mse: 0.0767 - mae: 0.2128 - val_loss: 0.1139 - val_mse: 0.1037 - val_mae: 0.2481\n",
      "Epoch 334/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0864 - mse: 0.0762 - mae: 0.2141 - val_loss: 0.1137 - val_mse: 0.1035 - val_mae: 0.2454\n",
      "Epoch 335/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0856 - mse: 0.0754 - mae: 0.2097 - val_loss: 0.1138 - val_mse: 0.1035 - val_mae: 0.2515\n",
      "Epoch 336/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0862 - mse: 0.0760 - mae: 0.2138 - val_loss: 0.1134 - val_mse: 0.1032 - val_mae: 0.2460\n",
      "Epoch 337/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0852 - mse: 0.0750 - mae: 0.2105 - val_loss: 0.1136 - val_mse: 0.1033 - val_mae: 0.2494\n",
      "Epoch 338/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0854 - mse: 0.0751 - mae: 0.2128 - val_loss: 0.1152 - val_mse: 0.1049 - val_mae: 0.2559\n",
      "Epoch 339/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0857 - mse: 0.0754 - mae: 0.2123 - val_loss: 0.1148 - val_mse: 0.1045 - val_mae: 0.2494\n",
      "Epoch 340/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0833 - mse: 0.0731 - mae: 0.2101 - val_loss: 0.1132 - val_mse: 0.1029 - val_mae: 0.2445\n",
      "Epoch 341/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0837 - mse: 0.0734 - mae: 0.2087 - val_loss: 0.1138 - val_mse: 0.1036 - val_mae: 0.2473\n",
      "Epoch 342/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0836 - mse: 0.0733 - mae: 0.2090 - val_loss: 0.1136 - val_mse: 0.1033 - val_mae: 0.2530\n",
      "Epoch 343/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0825 - mse: 0.0722 - mae: 0.2080 - val_loss: 0.1142 - val_mse: 0.1039 - val_mae: 0.2433\n",
      "Epoch 344/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0830 - mse: 0.0727 - mae: 0.2075 - val_loss: 0.1144 - val_mse: 0.1041 - val_mae: 0.2465\n",
      "Epoch 345/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0821 - mse: 0.0718 - mae: 0.2091 - val_loss: 0.1133 - val_mse: 0.1030 - val_mae: 0.2501\n",
      "Epoch 346/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0822 - mse: 0.0719 - mae: 0.2068 - val_loss: 0.1131 - val_mse: 0.1028 - val_mae: 0.2467\n",
      "Epoch 347/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0818 - mse: 0.0714 - mae: 0.2059 - val_loss: 0.1115 - val_mse: 0.1012 - val_mae: 0.2447\n",
      "Epoch 348/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0813 - mse: 0.0710 - mae: 0.2064 - val_loss: 0.1117 - val_mse: 0.1013 - val_mae: 0.2467\n",
      "Epoch 349/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0812 - mse: 0.0709 - mae: 0.2074 - val_loss: 0.1117 - val_mse: 0.1014 - val_mae: 0.2463\n",
      "Epoch 350/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0804 - mse: 0.0701 - mae: 0.2060 - val_loss: 0.1136 - val_mse: 0.1033 - val_mae: 0.2430\n",
      "Epoch 351/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0804 - mse: 0.0701 - mae: 0.2056 - val_loss: 0.1117 - val_mse: 0.1014 - val_mae: 0.2447\n",
      "Epoch 352/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0804 - mse: 0.0701 - mae: 0.2062 - val_loss: 0.1105 - val_mse: 0.1001 - val_mae: 0.2430\n",
      "Epoch 353/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0808 - mse: 0.0704 - mae: 0.2055 - val_loss: 0.1119 - val_mse: 0.1015 - val_mae: 0.2478\n",
      "Epoch 354/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0800 - mse: 0.0697 - mae: 0.2048 - val_loss: 0.1120 - val_mse: 0.1016 - val_mae: 0.2448\n",
      "Epoch 355/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0800 - mse: 0.0697 - mae: 0.2064 - val_loss: 0.1117 - val_mse: 0.1013 - val_mae: 0.2499\n",
      "Epoch 356/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0800 - mse: 0.0696 - mae: 0.2048 - val_loss: 0.1089 - val_mse: 0.0985 - val_mae: 0.2392\n",
      "Epoch 357/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0789 - mse: 0.0685 - mae: 0.2052 - val_loss: 0.1109 - val_mse: 0.1005 - val_mae: 0.2423\n",
      "Epoch 358/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0785 - mse: 0.0681 - mae: 0.2027 - val_loss: 0.1112 - val_mse: 0.1008 - val_mae: 0.2461\n",
      "Epoch 359/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0784 - mse: 0.0680 - mae: 0.2041 - val_loss: 0.1078 - val_mse: 0.0974 - val_mae: 0.2371\n",
      "Epoch 360/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0790 - mse: 0.0686 - mae: 0.2057 - val_loss: 0.1154 - val_mse: 0.1050 - val_mae: 0.2453\n",
      "Epoch 361/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0778 - mse: 0.0674 - mae: 0.2012 - val_loss: 0.1086 - val_mse: 0.0982 - val_mae: 0.2454\n",
      "Epoch 362/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0777 - mse: 0.0673 - mae: 0.2027 - val_loss: 0.1093 - val_mse: 0.0989 - val_mae: 0.2424\n",
      "Epoch 363/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0768 - mse: 0.0664 - mae: 0.2022 - val_loss: 0.1097 - val_mse: 0.0993 - val_mae: 0.2390\n",
      "Epoch 364/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0773 - mse: 0.0669 - mae: 0.2009 - val_loss: 0.1087 - val_mse: 0.0982 - val_mae: 0.2390\n",
      "Epoch 365/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0758 - mse: 0.0654 - mae: 0.1994 - val_loss: 0.1076 - val_mse: 0.0972 - val_mae: 0.2395\n",
      "Epoch 366/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0767 - mse: 0.0663 - mae: 0.2016 - val_loss: 0.1122 - val_mse: 0.1018 - val_mae: 0.2439\n",
      "Epoch 367/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0780 - mse: 0.0676 - mae: 0.2048 - val_loss: 0.1068 - val_mse: 0.0963 - val_mae: 0.2381\n",
      "Epoch 368/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0770 - mse: 0.0666 - mae: 0.1998 - val_loss: 0.1086 - val_mse: 0.0981 - val_mae: 0.2467\n",
      "Epoch 369/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0787 - mse: 0.0682 - mae: 0.2028 - val_loss: 0.1059 - val_mse: 0.0955 - val_mae: 0.2437\n",
      "Epoch 370/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0764 - mse: 0.0659 - mae: 0.2017 - val_loss: 0.1063 - val_mse: 0.0958 - val_mae: 0.2378\n",
      "Epoch 371/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0750 - mse: 0.0645 - mae: 0.1981 - val_loss: 0.1077 - val_mse: 0.0973 - val_mae: 0.2359\n",
      "Epoch 372/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0750 - mse: 0.0645 - mae: 0.2000 - val_loss: 0.1070 - val_mse: 0.0966 - val_mae: 0.2338\n",
      "Epoch 373/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0748 - mse: 0.0643 - mae: 0.1987 - val_loss: 0.1059 - val_mse: 0.0954 - val_mae: 0.2351\n",
      "Epoch 374/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0746 - mse: 0.0642 - mae: 0.1969 - val_loss: 0.1031 - val_mse: 0.0926 - val_mae: 0.2370\n",
      "Epoch 375/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0740 - mse: 0.0635 - mae: 0.1983 - val_loss: 0.1060 - val_mse: 0.0955 - val_mae: 0.2376\n",
      "Epoch 376/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0741 - mse: 0.0636 - mae: 0.1971 - val_loss: 0.1046 - val_mse: 0.0941 - val_mae: 0.2309\n",
      "Epoch 377/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0735 - mse: 0.0630 - mae: 0.1955 - val_loss: 0.1040 - val_mse: 0.0935 - val_mae: 0.2312\n",
      "Epoch 378/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0742 - mse: 0.0637 - mae: 0.1989 - val_loss: 0.1046 - val_mse: 0.0941 - val_mae: 0.2350\n",
      "Epoch 379/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0735 - mse: 0.0630 - mae: 0.1974 - val_loss: 0.1022 - val_mse: 0.0917 - val_mae: 0.2334\n",
      "Epoch 380/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0726 - mse: 0.0621 - mae: 0.1945 - val_loss: 0.1017 - val_mse: 0.0912 - val_mae: 0.2317\n",
      "Epoch 381/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0737 - mse: 0.0632 - mae: 0.1980 - val_loss: 0.1058 - val_mse: 0.0953 - val_mae: 0.2345\n",
      "Epoch 382/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0730 - mse: 0.0625 - mae: 0.1956 - val_loss: 0.1039 - val_mse: 0.0934 - val_mae: 0.2320\n",
      "Epoch 383/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0727 - mse: 0.0621 - mae: 0.1953 - val_loss: 0.1020 - val_mse: 0.0915 - val_mae: 0.2322\n",
      "Epoch 384/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - mse: 0.0610 - mae: 0.1945 - val_loss: 0.1035 - val_mse: 0.0930 - val_mae: 0.2384\n",
      "Epoch 385/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0720 - mse: 0.0615 - mae: 0.1942 - val_loss: 0.1004 - val_mse: 0.0899 - val_mae: 0.2319\n",
      "Epoch 386/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0715 - mse: 0.0609 - mae: 0.1944 - val_loss: 0.1056 - val_mse: 0.0951 - val_mae: 0.2341\n",
      "Epoch 387/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0733 - mse: 0.0627 - mae: 0.1958 - val_loss: 0.1017 - val_mse: 0.0912 - val_mae: 0.2283\n",
      "Epoch 388/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0722 - mse: 0.0617 - mae: 0.1971 - val_loss: 0.1022 - val_mse: 0.0917 - val_mae: 0.2311\n",
      "Epoch 389/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - mse: 0.0610 - mae: 0.1924 - val_loss: 0.1002 - val_mse: 0.0897 - val_mae: 0.2330\n",
      "Epoch 390/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0716 - mse: 0.0610 - mae: 0.1934 - val_loss: 0.0988 - val_mse: 0.0882 - val_mae: 0.2276\n",
      "Epoch 391/1000\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0887 - mse: 0.0782 - mae: 0.195 - 0s 2ms/step - loss: 0.0714 - mse: 0.0608 - mae: 0.1927 - val_loss: 0.1006 - val_mse: 0.0900 - val_mae: 0.2261\n",
      "Epoch 392/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0698 - mse: 0.0593 - mae: 0.1901 - val_loss: 0.0993 - val_mse: 0.0887 - val_mae: 0.2297\n",
      "Epoch 393/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0701 - mse: 0.0595 - mae: 0.1915 - val_loss: 0.0974 - val_mse: 0.0868 - val_mae: 0.2236\n",
      "Epoch 394/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0695 - mse: 0.0589 - mae: 0.1897 - val_loss: 0.0994 - val_mse: 0.0888 - val_mae: 0.2284\n",
      "Epoch 395/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0698 - mse: 0.0592 - mae: 0.1890 - val_loss: 0.0965 - val_mse: 0.0859 - val_mae: 0.2240\n",
      "Epoch 396/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0698 - mse: 0.0592 - mae: 0.1882 - val_loss: 0.0984 - val_mse: 0.0878 - val_mae: 0.2330\n",
      "Epoch 397/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0700 - mse: 0.0594 - mae: 0.1903 - val_loss: 0.0969 - val_mse: 0.0862 - val_mae: 0.2224\n",
      "Epoch 398/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0689 - mse: 0.0583 - mae: 0.1884 - val_loss: 0.0951 - val_mse: 0.0845 - val_mae: 0.2217\n",
      "Epoch 399/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0689 - mse: 0.0582 - mae: 0.1883 - val_loss: 0.0968 - val_mse: 0.0862 - val_mae: 0.2214\n",
      "Epoch 400/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0681 - mse: 0.0575 - mae: 0.1875 - val_loss: 0.0960 - val_mse: 0.0854 - val_mae: 0.2275\n",
      "Epoch 401/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0684 - mse: 0.0577 - mae: 0.1885 - val_loss: 0.0956 - val_mse: 0.0850 - val_mae: 0.2170\n",
      "Epoch 402/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0705 - mse: 0.0599 - mae: 0.1915 - val_loss: 0.1004 - val_mse: 0.0898 - val_mae: 0.2337\n",
      "Epoch 403/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0700 - mse: 0.0593 - mae: 0.1895 - val_loss: 0.0943 - val_mse: 0.0836 - val_mae: 0.2234\n",
      "Epoch 404/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0685 - mse: 0.0578 - mae: 0.1885 - val_loss: 0.0967 - val_mse: 0.0860 - val_mae: 0.2243\n",
      "Epoch 405/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0687 - mse: 0.0580 - mae: 0.1891 - val_loss: 0.0985 - val_mse: 0.0878 - val_mae: 0.2203\n",
      "Epoch 406/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0690 - mse: 0.0583 - mae: 0.1892 - val_loss: 0.0937 - val_mse: 0.0830 - val_mae: 0.2214\n",
      "Epoch 407/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0670 - mse: 0.0563 - mae: 0.1847 - val_loss: 0.0961 - val_mse: 0.0854 - val_mae: 0.2304\n",
      "Epoch 408/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0693 - mse: 0.0586 - mae: 0.1895 - val_loss: 0.0947 - val_mse: 0.0841 - val_mae: 0.2225\n",
      "Epoch 409/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0686 - mse: 0.0579 - mae: 0.1880 - val_loss: 0.0947 - val_mse: 0.0840 - val_mae: 0.2181\n",
      "Epoch 410/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0672 - mse: 0.0565 - mae: 0.1861 - val_loss: 0.0926 - val_mse: 0.0819 - val_mae: 0.2197\n",
      "Epoch 411/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0665 - mse: 0.0558 - mae: 0.1833 - val_loss: 0.0921 - val_mse: 0.0814 - val_mae: 0.2185\n",
      "Epoch 412/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0676 - mse: 0.0568 - mae: 0.1881 - val_loss: 0.0925 - val_mse: 0.0818 - val_mae: 0.2164\n",
      "Epoch 413/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0675 - mse: 0.0568 - mae: 0.1854 - val_loss: 0.0949 - val_mse: 0.0842 - val_mae: 0.2149\n",
      "Epoch 414/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0661 - mse: 0.0554 - mae: 0.1835 - val_loss: 0.0913 - val_mse: 0.0806 - val_mae: 0.2173\n",
      "Epoch 415/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0660 - mse: 0.0553 - mae: 0.1826 - val_loss: 0.0912 - val_mse: 0.0804 - val_mae: 0.2142\n",
      "Epoch 416/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0655 - mse: 0.0548 - mae: 0.1835 - val_loss: 0.0919 - val_mse: 0.0812 - val_mae: 0.2146\n",
      "Epoch 417/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0663 - mse: 0.0556 - mae: 0.1845 - val_loss: 0.0935 - val_mse: 0.0827 - val_mae: 0.2180\n",
      "Epoch 418/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0663 - mse: 0.0555 - mae: 0.1843 - val_loss: 0.0908 - val_mse: 0.0801 - val_mae: 0.2085\n",
      "Epoch 419/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0660 - mse: 0.0552 - mae: 0.1834 - val_loss: 0.0886 - val_mse: 0.0778 - val_mae: 0.2139\n",
      "Epoch 420/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0647 - mse: 0.0539 - mae: 0.1811 - val_loss: 0.0895 - val_mse: 0.0787 - val_mae: 0.2104\n",
      "Epoch 421/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0656 - mse: 0.0548 - mae: 0.1824 - val_loss: 0.0900 - val_mse: 0.0792 - val_mae: 0.2153\n",
      "Epoch 422/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0650 - mse: 0.0542 - mae: 0.1802 - val_loss: 0.0897 - val_mse: 0.0789 - val_mae: 0.2185\n",
      "Epoch 423/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0647 - mse: 0.0539 - mae: 0.1811 - val_loss: 0.0898 - val_mse: 0.0790 - val_mae: 0.2113\n",
      "Epoch 424/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0648 - mse: 0.0540 - mae: 0.1812 - val_loss: 0.0906 - val_mse: 0.0798 - val_mae: 0.2135\n",
      "Epoch 425/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0645 - mse: 0.0537 - mae: 0.1795 - val_loss: 0.0900 - val_mse: 0.0792 - val_mae: 0.2137\n",
      "Epoch 426/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0658 - mse: 0.0550 - mae: 0.1838 - val_loss: 0.0887 - val_mse: 0.0779 - val_mae: 0.2127\n",
      "Epoch 427/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0662 - mse: 0.0554 - mae: 0.1827 - val_loss: 0.0879 - val_mse: 0.0771 - val_mae: 0.2142\n",
      "Epoch 428/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0648 - mse: 0.0540 - mae: 0.1826 - val_loss: 0.0881 - val_mse: 0.0773 - val_mae: 0.2160\n",
      "Epoch 429/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0634 - mse: 0.0526 - mae: 0.1780 - val_loss: 0.0894 - val_mse: 0.0786 - val_mae: 0.2144\n",
      "Epoch 430/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0646 - mse: 0.0538 - mae: 0.1810 - val_loss: 0.0882 - val_mse: 0.0774 - val_mae: 0.2171\n",
      "Epoch 431/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0634 - mse: 0.0526 - mae: 0.1809 - val_loss: 0.0879 - val_mse: 0.0771 - val_mae: 0.2088\n",
      "Epoch 432/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0640 - mse: 0.0532 - mae: 0.1786 - val_loss: 0.0882 - val_mse: 0.0774 - val_mae: 0.2173\n",
      "Epoch 433/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0641 - mse: 0.0533 - mae: 0.1817 - val_loss: 0.0858 - val_mse: 0.0750 - val_mae: 0.2068\n",
      "Epoch 434/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0641 - mse: 0.0533 - mae: 0.1801 - val_loss: 0.0867 - val_mse: 0.0759 - val_mae: 0.2051\n",
      "Epoch 435/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0629 - mse: 0.0521 - mae: 0.1770 - val_loss: 0.0897 - val_mse: 0.0788 - val_mae: 0.2136\n",
      "Epoch 436/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0639 - mse: 0.0531 - mae: 0.1794 - val_loss: 0.0853 - val_mse: 0.0745 - val_mae: 0.2058\n",
      "Epoch 437/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0632 - mse: 0.0524 - mae: 0.1793 - val_loss: 0.0860 - val_mse: 0.0751 - val_mae: 0.2107\n",
      "Epoch 438/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0641 - mse: 0.0532 - mae: 0.1788 - val_loss: 0.0862 - val_mse: 0.0753 - val_mae: 0.2112\n",
      "Epoch 439/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0632 - mse: 0.0523 - mae: 0.1796 - val_loss: 0.0858 - val_mse: 0.0750 - val_mae: 0.2088\n",
      "Epoch 440/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0626 - mse: 0.0518 - mae: 0.1770 - val_loss: 0.0850 - val_mse: 0.0741 - val_mae: 0.2098\n",
      "Epoch 441/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0636 - mse: 0.0528 - mae: 0.1782 - val_loss: 0.0847 - val_mse: 0.0738 - val_mae: 0.2061\n",
      "Epoch 442/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0623 - mse: 0.0515 - mae: 0.1770 - val_loss: 0.0868 - val_mse: 0.0760 - val_mae: 0.2091\n",
      "Epoch 443/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0621 - mse: 0.0512 - mae: 0.1753 - val_loss: 0.0832 - val_mse: 0.0723 - val_mae: 0.2049\n",
      "Epoch 444/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0623 - mse: 0.0514 - mae: 0.1769 - val_loss: 0.0848 - val_mse: 0.0739 - val_mae: 0.2056\n",
      "Epoch 445/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0619 - mse: 0.0511 - mae: 0.1750 - val_loss: 0.0847 - val_mse: 0.0738 - val_mae: 0.2098\n",
      "Epoch 446/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0627 - mse: 0.0518 - mae: 0.1783 - val_loss: 0.0831 - val_mse: 0.0723 - val_mae: 0.2040\n",
      "Epoch 447/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0630 - mse: 0.0522 - mae: 0.1797 - val_loss: 0.0881 - val_mse: 0.0772 - val_mae: 0.2072\n",
      "Epoch 448/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0640 - mse: 0.0531 - mae: 0.1778 - val_loss: 0.0857 - val_mse: 0.0748 - val_mae: 0.2109\n",
      "Epoch 449/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0628 - mse: 0.0519 - mae: 0.1765 - val_loss: 0.0837 - val_mse: 0.0728 - val_mae: 0.2042\n",
      "Epoch 450/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0621 - mse: 0.0512 - mae: 0.1760 - val_loss: 0.0865 - val_mse: 0.0756 - val_mae: 0.2073\n",
      "Epoch 451/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0612 - mse: 0.0503 - mae: 0.1738 - val_loss: 0.0836 - val_mse: 0.0727 - val_mae: 0.2058\n",
      "Epoch 452/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0622 - mse: 0.0513 - mae: 0.1780 - val_loss: 0.0837 - val_mse: 0.0728 - val_mae: 0.2080\n",
      "Epoch 453/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0617 - mse: 0.0508 - mae: 0.1770 - val_loss: 0.0822 - val_mse: 0.0713 - val_mae: 0.2024\n",
      "Epoch 454/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0620 - mse: 0.0511 - mae: 0.1736 - val_loss: 0.0826 - val_mse: 0.0717 - val_mae: 0.2008\n",
      "Epoch 455/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0613 - mse: 0.0504 - mae: 0.1745 - val_loss: 0.0875 - val_mse: 0.0766 - val_mae: 0.2078\n",
      "Epoch 456/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0609 - mse: 0.0500 - mae: 0.1719 - val_loss: 0.0824 - val_mse: 0.0714 - val_mae: 0.2019\n",
      "Epoch 457/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0605 - mse: 0.0496 - mae: 0.1724 - val_loss: 0.0804 - val_mse: 0.0695 - val_mae: 0.1997\n",
      "Epoch 458/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0605 - mse: 0.0496 - mae: 0.1738 - val_loss: 0.0809 - val_mse: 0.0700 - val_mae: 0.1996\n",
      "Epoch 459/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0618 - mse: 0.0508 - mae: 0.1739 - val_loss: 0.0820 - val_mse: 0.0711 - val_mae: 0.2060\n",
      "Epoch 460/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0606 - mse: 0.0496 - mae: 0.1733 - val_loss: 0.0837 - val_mse: 0.0728 - val_mae: 0.2019\n",
      "Epoch 461/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0614 - mse: 0.0504 - mae: 0.1726 - val_loss: 0.0823 - val_mse: 0.0714 - val_mae: 0.2049\n",
      "Epoch 462/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0601 - mse: 0.0491 - mae: 0.1712 - val_loss: 0.0786 - val_mse: 0.0677 - val_mae: 0.1943\n",
      "Epoch 463/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0594 - mse: 0.0484 - mae: 0.1690 - val_loss: 0.0810 - val_mse: 0.0700 - val_mae: 0.2034\n",
      "Epoch 464/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0602 - mse: 0.0492 - mae: 0.1716 - val_loss: 0.0797 - val_mse: 0.0688 - val_mae: 0.2013\n",
      "Epoch 465/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0594 - mse: 0.0484 - mae: 0.1690 - val_loss: 0.0792 - val_mse: 0.0683 - val_mae: 0.1969\n",
      "Epoch 466/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0603 - mse: 0.0493 - mae: 0.1713 - val_loss: 0.0790 - val_mse: 0.0680 - val_mae: 0.1967\n",
      "Epoch 467/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0590 - mse: 0.0480 - mae: 0.1688 - val_loss: 0.0787 - val_mse: 0.0677 - val_mae: 0.1974\n",
      "Epoch 468/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0591 - mse: 0.0481 - mae: 0.1693 - val_loss: 0.0813 - val_mse: 0.0704 - val_mae: 0.1982\n",
      "Epoch 469/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0593 - mse: 0.0483 - mae: 0.1682 - val_loss: 0.0802 - val_mse: 0.0692 - val_mae: 0.2015\n",
      "Epoch 470/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0609 - mse: 0.0499 - mae: 0.1714 - val_loss: 0.0779 - val_mse: 0.0669 - val_mae: 0.1982\n",
      "Epoch 471/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0590 - mse: 0.0480 - mae: 0.1684 - val_loss: 0.0784 - val_mse: 0.0674 - val_mae: 0.1991\n",
      "Epoch 472/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0587 - mse: 0.0477 - mae: 0.1671 - val_loss: 0.0765 - val_mse: 0.0655 - val_mae: 0.1945\n",
      "Epoch 473/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0583 - mse: 0.0473 - mae: 0.1665 - val_loss: 0.0810 - val_mse: 0.0700 - val_mae: 0.1992\n",
      "Epoch 474/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0594 - mse: 0.0484 - mae: 0.1700 - val_loss: 0.0816 - val_mse: 0.0706 - val_mae: 0.1983\n",
      "Epoch 475/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0591 - mse: 0.0480 - mae: 0.1671 - val_loss: 0.0767 - val_mse: 0.0657 - val_mae: 0.1979\n",
      "Epoch 476/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0580 - mse: 0.0470 - mae: 0.1682 - val_loss: 0.0775 - val_mse: 0.0665 - val_mae: 0.1925\n",
      "Epoch 477/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0582 - mse: 0.0472 - mae: 0.1662 - val_loss: 0.0773 - val_mse: 0.0662 - val_mae: 0.1935\n",
      "Epoch 478/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0575 - mse: 0.0465 - mae: 0.1659 - val_loss: 0.0763 - val_mse: 0.0653 - val_mae: 0.1916\n",
      "Epoch 479/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0576 - mse: 0.0466 - mae: 0.1671 - val_loss: 0.0761 - val_mse: 0.0650 - val_mae: 0.1893\n",
      "Epoch 480/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0574 - mse: 0.0463 - mae: 0.1636 - val_loss: 0.0767 - val_mse: 0.0657 - val_mae: 0.1929\n",
      "Epoch 481/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0573 - mse: 0.0462 - mae: 0.1669 - val_loss: 0.0764 - val_mse: 0.0653 - val_mae: 0.1926\n",
      "Epoch 482/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0570 - mse: 0.0459 - mae: 0.1647 - val_loss: 0.0780 - val_mse: 0.0669 - val_mae: 0.1949\n",
      "Epoch 483/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0566 - mse: 0.0456 - mae: 0.1634 - val_loss: 0.0747 - val_mse: 0.0637 - val_mae: 0.1900\n",
      "Epoch 484/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0568 - mse: 0.0457 - mae: 0.1637 - val_loss: 0.0766 - val_mse: 0.0655 - val_mae: 0.1933\n",
      "Epoch 485/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0565 - mse: 0.0454 - mae: 0.1631 - val_loss: 0.0768 - val_mse: 0.0657 - val_mae: 0.1945\n",
      "Epoch 486/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0568 - mse: 0.0457 - mae: 0.1631 - val_loss: 0.0767 - val_mse: 0.0656 - val_mae: 0.1892\n",
      "Epoch 487/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0571 - mse: 0.0461 - mae: 0.1652 - val_loss: 0.0759 - val_mse: 0.0649 - val_mae: 0.1940\n",
      "Epoch 488/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0565 - mse: 0.0454 - mae: 0.1636 - val_loss: 0.0739 - val_mse: 0.0628 - val_mae: 0.1864\n",
      "Epoch 489/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0565 - mse: 0.0454 - mae: 0.1615 - val_loss: 0.0750 - val_mse: 0.0639 - val_mae: 0.1928\n",
      "Epoch 490/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0563 - mse: 0.0452 - mae: 0.1642 - val_loss: 0.0746 - val_mse: 0.0635 - val_mae: 0.1881\n",
      "Epoch 491/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0570 - mse: 0.0459 - mae: 0.1668 - val_loss: 0.0769 - val_mse: 0.0658 - val_mae: 0.1927\n",
      "Epoch 492/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0566 - mse: 0.0455 - mae: 0.1629 - val_loss: 0.0756 - val_mse: 0.0645 - val_mae: 0.1930\n",
      "Epoch 493/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0555 - mse: 0.0444 - mae: 0.1617 - val_loss: 0.0743 - val_mse: 0.0632 - val_mae: 0.1866\n",
      "Epoch 494/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0558 - mse: 0.0447 - mae: 0.1618 - val_loss: 0.0741 - val_mse: 0.0630 - val_mae: 0.1900\n",
      "Epoch 495/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0559 - mse: 0.0448 - mae: 0.1639 - val_loss: 0.0725 - val_mse: 0.0614 - val_mae: 0.1878\n",
      "Epoch 496/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0555 - mse: 0.0444 - mae: 0.1613 - val_loss: 0.0727 - val_mse: 0.0616 - val_mae: 0.1884\n",
      "Epoch 497/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0551 - mse: 0.0440 - mae: 0.1617 - val_loss: 0.0733 - val_mse: 0.0622 - val_mae: 0.1830\n",
      "Epoch 498/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0552 - mse: 0.0440 - mae: 0.1604 - val_loss: 0.0738 - val_mse: 0.0627 - val_mae: 0.1873\n",
      "Epoch 499/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0550 - mse: 0.0439 - mae: 0.1619 - val_loss: 0.0786 - val_mse: 0.0675 - val_mae: 0.1963\n",
      "Epoch 500/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0565 - mse: 0.0453 - mae: 0.1642 - val_loss: 0.0746 - val_mse: 0.0635 - val_mae: 0.1881\n",
      "Epoch 501/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0546 - mse: 0.0435 - mae: 0.1598 - val_loss: 0.0720 - val_mse: 0.0609 - val_mae: 0.1882\n",
      "Epoch 502/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0550 - mse: 0.0438 - mae: 0.1581 - val_loss: 0.0717 - val_mse: 0.0606 - val_mae: 0.1879\n",
      "Epoch 503/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0550 - mse: 0.0439 - mae: 0.1618 - val_loss: 0.0714 - val_mse: 0.0603 - val_mae: 0.1844\n",
      "Epoch 504/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0539 - mse: 0.0428 - mae: 0.1593 - val_loss: 0.0735 - val_mse: 0.0624 - val_mae: 0.1854\n",
      "Epoch 505/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0546 - mse: 0.0434 - mae: 0.1601 - val_loss: 0.0732 - val_mse: 0.0620 - val_mae: 0.1842\n",
      "Epoch 506/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0544 - mse: 0.0433 - mae: 0.1582 - val_loss: 0.0712 - val_mse: 0.0600 - val_mae: 0.1847\n",
      "Epoch 507/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0540 - mse: 0.0428 - mae: 0.1588 - val_loss: 0.0704 - val_mse: 0.0592 - val_mae: 0.1833\n",
      "Epoch 508/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0538 - mse: 0.0427 - mae: 0.1583 - val_loss: 0.0710 - val_mse: 0.0598 - val_mae: 0.1822\n",
      "Epoch 509/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0538 - mse: 0.0427 - mae: 0.1582 - val_loss: 0.0710 - val_mse: 0.0599 - val_mae: 0.1802\n",
      "Epoch 510/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0546 - mse: 0.0435 - mae: 0.1597 - val_loss: 0.0730 - val_mse: 0.0619 - val_mae: 0.1922\n",
      "Epoch 511/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0541 - mse: 0.0429 - mae: 0.1584 - val_loss: 0.0700 - val_mse: 0.0588 - val_mae: 0.1803\n",
      "Epoch 512/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0542 - mse: 0.0431 - mae: 0.1588 - val_loss: 0.0699 - val_mse: 0.0587 - val_mae: 0.1808\n",
      "Epoch 513/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0534 - mse: 0.0422 - mae: 0.1569 - val_loss: 0.0733 - val_mse: 0.0621 - val_mae: 0.1857\n",
      "Epoch 514/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0534 - mse: 0.0423 - mae: 0.1566 - val_loss: 0.0718 - val_mse: 0.0606 - val_mae: 0.1810\n",
      "Epoch 515/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0535 - mse: 0.0424 - mae: 0.1589 - val_loss: 0.0696 - val_mse: 0.0585 - val_mae: 0.1802\n",
      "Epoch 516/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0540 - mse: 0.0428 - mae: 0.1565 - val_loss: 0.0720 - val_mse: 0.0608 - val_mae: 0.1820\n",
      "Epoch 517/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0531 - mse: 0.0419 - mae: 0.1562 - val_loss: 0.0761 - val_mse: 0.0650 - val_mae: 0.1917\n",
      "Epoch 518/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0535 - mse: 0.0423 - mae: 0.1561 - val_loss: 0.0695 - val_mse: 0.0583 - val_mae: 0.1771\n",
      "Epoch 519/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0530 - mse: 0.0419 - mae: 0.1544 - val_loss: 0.0701 - val_mse: 0.0589 - val_mae: 0.1852\n",
      "Epoch 520/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0534 - mse: 0.0422 - mae: 0.1576 - val_loss: 0.0686 - val_mse: 0.0574 - val_mae: 0.1755\n",
      "Epoch 521/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0536 - mse: 0.0424 - mae: 0.1546 - val_loss: 0.0706 - val_mse: 0.0594 - val_mae: 0.1854\n",
      "Epoch 522/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0536 - mse: 0.0424 - mae: 0.1571 - val_loss: 0.0692 - val_mse: 0.0580 - val_mae: 0.1790\n",
      "Epoch 523/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0526 - mse: 0.0414 - mae: 0.1544 - val_loss: 0.0696 - val_mse: 0.0584 - val_mae: 0.1770\n",
      "Epoch 524/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0527 - mse: 0.0415 - mae: 0.1567 - val_loss: 0.0706 - val_mse: 0.0594 - val_mae: 0.1807\n",
      "Epoch 525/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0531 - mse: 0.0419 - mae: 0.1548 - val_loss: 0.0691 - val_mse: 0.0579 - val_mae: 0.1834\n",
      "Epoch 526/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0522 - mse: 0.0410 - mae: 0.1545 - val_loss: 0.0702 - val_mse: 0.0590 - val_mae: 0.1794\n",
      "Epoch 527/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0523 - mse: 0.0411 - mae: 0.1536 - val_loss: 0.0685 - val_mse: 0.0573 - val_mae: 0.1784\n",
      "Epoch 528/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0522 - mse: 0.0410 - mae: 0.1536 - val_loss: 0.0683 - val_mse: 0.0571 - val_mae: 0.1801\n",
      "Epoch 529/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0527 - mse: 0.0415 - mae: 0.1542 - val_loss: 0.0671 - val_mse: 0.0559 - val_mae: 0.1739\n",
      "Epoch 530/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0520 - mse: 0.0408 - mae: 0.1544 - val_loss: 0.0700 - val_mse: 0.0587 - val_mae: 0.1823\n",
      "Epoch 531/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0520 - mse: 0.0408 - mae: 0.1543 - val_loss: 0.0721 - val_mse: 0.0609 - val_mae: 0.1839\n",
      "Epoch 532/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0536 - mse: 0.0424 - mae: 0.1579 - val_loss: 0.0747 - val_mse: 0.0635 - val_mae: 0.1954\n",
      "Epoch 533/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0532 - mse: 0.0420 - mae: 0.1590 - val_loss: 0.0661 - val_mse: 0.0549 - val_mae: 0.1710\n",
      "Epoch 534/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0516 - mse: 0.0404 - mae: 0.1531 - val_loss: 0.0660 - val_mse: 0.0548 - val_mae: 0.1750\n",
      "Epoch 535/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0516 - mse: 0.0404 - mae: 0.1529 - val_loss: 0.0658 - val_mse: 0.0546 - val_mae: 0.1714\n",
      "Epoch 536/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0514 - mse: 0.0402 - mae: 0.1528 - val_loss: 0.0688 - val_mse: 0.0576 - val_mae: 0.1804\n",
      "Epoch 537/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0516 - mse: 0.0403 - mae: 0.1527 - val_loss: 0.0667 - val_mse: 0.0554 - val_mae: 0.1776\n",
      "Epoch 538/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0514 - mse: 0.0402 - mae: 0.1518 - val_loss: 0.0649 - val_mse: 0.0537 - val_mae: 0.1713\n",
      "Epoch 539/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0511 - mse: 0.0398 - mae: 0.1509 - val_loss: 0.0677 - val_mse: 0.0565 - val_mae: 0.1792\n",
      "Epoch 540/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0514 - mse: 0.0401 - mae: 0.1516 - val_loss: 0.0688 - val_mse: 0.0576 - val_mae: 0.1787\n",
      "Epoch 541/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0508 - mse: 0.0396 - mae: 0.1507 - val_loss: 0.0680 - val_mse: 0.0567 - val_mae: 0.1798\n",
      "Epoch 542/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0505 - mse: 0.0393 - mae: 0.1511 - val_loss: 0.0655 - val_mse: 0.0542 - val_mae: 0.1711\n",
      "Epoch 543/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0503 - mse: 0.0391 - mae: 0.1496 - val_loss: 0.0700 - val_mse: 0.0588 - val_mae: 0.1826\n",
      "Epoch 544/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0510 - mse: 0.0397 - mae: 0.1499 - val_loss: 0.0646 - val_mse: 0.0533 - val_mae: 0.1708\n",
      "Epoch 545/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0507 - mse: 0.0394 - mae: 0.1512 - val_loss: 0.0656 - val_mse: 0.0544 - val_mae: 0.1736\n",
      "Epoch 546/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0497 - mse: 0.0385 - mae: 0.1484 - val_loss: 0.0664 - val_mse: 0.0552 - val_mae: 0.1753\n",
      "Epoch 547/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0499 - mse: 0.0387 - mae: 0.1484 - val_loss: 0.0642 - val_mse: 0.0529 - val_mae: 0.1688\n",
      "Epoch 548/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0505 - mse: 0.0392 - mae: 0.1500 - val_loss: 0.0639 - val_mse: 0.0527 - val_mae: 0.1689\n",
      "Epoch 549/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0494 - mse: 0.0381 - mae: 0.1480 - val_loss: 0.0652 - val_mse: 0.0539 - val_mae: 0.1705\n",
      "Epoch 550/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0494 - mse: 0.0381 - mae: 0.1462 - val_loss: 0.0650 - val_mse: 0.0537 - val_mae: 0.1764\n",
      "Epoch 551/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0506 - mse: 0.0394 - mae: 0.1508 - val_loss: 0.0639 - val_mse: 0.0526 - val_mae: 0.1669\n",
      "Epoch 552/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0490 - mse: 0.0377 - mae: 0.1465 - val_loss: 0.0643 - val_mse: 0.0530 - val_mae: 0.1705\n",
      "Epoch 553/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0499 - mse: 0.0386 - mae: 0.1484 - val_loss: 0.0639 - val_mse: 0.0526 - val_mae: 0.1697\n",
      "Epoch 554/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0492 - mse: 0.0379 - mae: 0.1470 - val_loss: 0.0631 - val_mse: 0.0518 - val_mae: 0.1656\n",
      "Epoch 555/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0491 - mse: 0.0378 - mae: 0.1477 - val_loss: 0.0630 - val_mse: 0.0517 - val_mae: 0.1673\n",
      "Epoch 556/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0497 - mse: 0.0384 - mae: 0.1473 - val_loss: 0.0629 - val_mse: 0.0516 - val_mae: 0.1659\n",
      "Epoch 557/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0491 - mse: 0.0378 - mae: 0.1483 - val_loss: 0.0634 - val_mse: 0.0521 - val_mae: 0.1676\n",
      "Epoch 558/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0490 - mse: 0.0376 - mae: 0.1455 - val_loss: 0.0649 - val_mse: 0.0536 - val_mae: 0.1731\n",
      "Epoch 559/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0483 - mse: 0.0370 - mae: 0.1449 - val_loss: 0.0645 - val_mse: 0.0532 - val_mae: 0.1677\n",
      "Epoch 560/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0487 - mse: 0.0374 - mae: 0.1450 - val_loss: 0.0649 - val_mse: 0.0536 - val_mae: 0.1751\n",
      "Epoch 561/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0485 - mse: 0.0372 - mae: 0.1459 - val_loss: 0.0642 - val_mse: 0.0529 - val_mae: 0.1695\n",
      "Epoch 562/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0483 - mse: 0.0370 - mae: 0.1449 - val_loss: 0.0642 - val_mse: 0.0529 - val_mae: 0.1717\n",
      "Epoch 563/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0490 - mse: 0.0376 - mae: 0.1454 - val_loss: 0.0644 - val_mse: 0.0531 - val_mae: 0.1740\n",
      "Epoch 564/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0485 - mse: 0.0372 - mae: 0.1451 - val_loss: 0.0625 - val_mse: 0.0512 - val_mae: 0.1659\n",
      "Epoch 565/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0482 - mse: 0.0368 - mae: 0.1440 - val_loss: 0.0631 - val_mse: 0.0518 - val_mae: 0.1685\n",
      "Epoch 566/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0483 - mse: 0.0369 - mae: 0.1439 - val_loss: 0.0681 - val_mse: 0.0567 - val_mae: 0.1793\n",
      "Epoch 567/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0484 - mse: 0.0371 - mae: 0.1437 - val_loss: 0.0646 - val_mse: 0.0532 - val_mae: 0.1708\n",
      "Epoch 568/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0487 - mse: 0.0373 - mae: 0.1460 - val_loss: 0.0636 - val_mse: 0.0523 - val_mae: 0.1658\n",
      "Epoch 569/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0479 - mse: 0.0365 - mae: 0.1450 - val_loss: 0.0659 - val_mse: 0.0546 - val_mae: 0.1744\n",
      "Epoch 570/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0486 - mse: 0.0372 - mae: 0.1461 - val_loss: 0.0641 - val_mse: 0.0528 - val_mae: 0.1707\n",
      "Epoch 571/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0475 - mse: 0.0362 - mae: 0.1438 - val_loss: 0.0666 - val_mse: 0.0552 - val_mae: 0.1777\n",
      "Epoch 572/1000\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0484 - mse: 0.0371 - mae: 0.1440 - val_loss: 0.0643 - val_mse: 0.0530 - val_mae: 0.1716\n",
      "Epoch 573/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0479 - mse: 0.0365 - mae: 0.1440 - val_loss: 0.0672 - val_mse: 0.0558 - val_mae: 0.1801\n",
      "Epoch 574/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0478 - mse: 0.0364 - mae: 0.1428 - val_loss: 0.0631 - val_mse: 0.0518 - val_mae: 0.1664\n",
      "Epoch 575/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0472 - mse: 0.0358 - mae: 0.1418 - val_loss: 0.0612 - val_mse: 0.0498 - val_mae: 0.1638\n",
      "Epoch 576/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0469 - mse: 0.0355 - mae: 0.1410 - val_loss: 0.0606 - val_mse: 0.0492 - val_mae: 0.1627\n",
      "Epoch 577/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0470 - mse: 0.0357 - mae: 0.1417 - val_loss: 0.0619 - val_mse: 0.0505 - val_mae: 0.1647\n",
      "Epoch 578/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0471 - mse: 0.0358 - mae: 0.1413 - val_loss: 0.0601 - val_mse: 0.0487 - val_mae: 0.1613\n",
      "Epoch 579/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0483 - mse: 0.0369 - mae: 0.1431 - val_loss: 0.0619 - val_mse: 0.0505 - val_mae: 0.1652\n",
      "Epoch 580/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0464 - mse: 0.0350 - mae: 0.1401 - val_loss: 0.0610 - val_mse: 0.0497 - val_mae: 0.1614\n",
      "Epoch 581/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0465 - mse: 0.0351 - mae: 0.1396 - val_loss: 0.0627 - val_mse: 0.0514 - val_mae: 0.1663\n",
      "Epoch 582/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0470 - mse: 0.0357 - mae: 0.1403 - val_loss: 0.0605 - val_mse: 0.0491 - val_mae: 0.1663\n",
      "Epoch 583/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0466 - mse: 0.0352 - mae: 0.1393 - val_loss: 0.0603 - val_mse: 0.0489 - val_mae: 0.1632\n",
      "Epoch 584/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0462 - mse: 0.0348 - mae: 0.1396 - val_loss: 0.0615 - val_mse: 0.0501 - val_mae: 0.1625\n",
      "Epoch 585/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0467 - mse: 0.0353 - mae: 0.1405 - val_loss: 0.0627 - val_mse: 0.0514 - val_mae: 0.1706\n",
      "Epoch 586/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0463 - mse: 0.0349 - mae: 0.1385 - val_loss: 0.0623 - val_mse: 0.0509 - val_mae: 0.1684\n",
      "Epoch 587/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0475 - mse: 0.0361 - mae: 0.1449 - val_loss: 0.0611 - val_mse: 0.0497 - val_mae: 0.1644\n",
      "Epoch 588/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0465 - mse: 0.0351 - mae: 0.1398 - val_loss: 0.0608 - val_mse: 0.0494 - val_mae: 0.1606\n",
      "Epoch 589/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0464 - mse: 0.0349 - mae: 0.1388 - val_loss: 0.0606 - val_mse: 0.0492 - val_mae: 0.1628\n",
      "Epoch 590/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0458 - mse: 0.0344 - mae: 0.1373 - val_loss: 0.0602 - val_mse: 0.0488 - val_mae: 0.1608\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0456 - mse: 0.0342 - mae: 0.1381 - val_loss: 0.0598 - val_mse: 0.0484 - val_mae: 0.1603\n",
      "Epoch 592/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0457 - mse: 0.0343 - mae: 0.1362 - val_loss: 0.0599 - val_mse: 0.0485 - val_mae: 0.1610\n",
      "Epoch 593/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0456 - mse: 0.0342 - mae: 0.1386 - val_loss: 0.0628 - val_mse: 0.0514 - val_mae: 0.1696\n",
      "Epoch 594/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0463 - mse: 0.0349 - mae: 0.1388 - val_loss: 0.0611 - val_mse: 0.0497 - val_mae: 0.1685\n",
      "Epoch 595/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0460 - mse: 0.0346 - mae: 0.1403 - val_loss: 0.0607 - val_mse: 0.0493 - val_mae: 0.1624\n",
      "Epoch 596/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0458 - mse: 0.0343 - mae: 0.1370 - val_loss: 0.0598 - val_mse: 0.0484 - val_mae: 0.1621\n",
      "Epoch 597/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0460 - mse: 0.0346 - mae: 0.1392 - val_loss: 0.0609 - val_mse: 0.0495 - val_mae: 0.1647\n",
      "Epoch 598/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0464 - mse: 0.0350 - mae: 0.1368 - val_loss: 0.0607 - val_mse: 0.0492 - val_mae: 0.1613\n",
      "Epoch 599/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0462 - mse: 0.0348 - mae: 0.1391 - val_loss: 0.0586 - val_mse: 0.0471 - val_mae: 0.1611\n",
      "Epoch 600/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0460 - mse: 0.0346 - mae: 0.1404 - val_loss: 0.0601 - val_mse: 0.0486 - val_mae: 0.1595\n",
      "Epoch 601/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0452 - mse: 0.0337 - mae: 0.1373 - val_loss: 0.0631 - val_mse: 0.0517 - val_mae: 0.1691\n",
      "Epoch 602/1000\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0458 - mse: 0.0344 - mae: 0.1365 - val_loss: 0.0596 - val_mse: 0.0482 - val_mae: 0.1597\n"
     ]
    }
   ],
   "source": [
    "############################  TRAINING ############################ \n",
    "\n",
    "training_time = time.time()\n",
    "\n",
    "history = model.fit(Xtrain, ytrain, validation_data=(Xval, yval),\n",
    "                    epochs=epochs, verbose=1)\n",
    "\n",
    "# history = model.fit(Xtrain, ytrain, validation_data=(Xval, yval),\n",
    "#                     epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "training_time = training_time - time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b32de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire model to a HDF5 file\n",
    "model.save(filename_out + '_model.h5')\n",
    "# Save model to XML\n",
    "utils.save_model_to_xml(filename_out + '.xml', model, Xpeaks, ypeaks, scaleInput, normalizeOutput)\n",
    "# plot predictions\n",
    "y_pred = model.predict(Xnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c78dd",
   "metadata": {},
   "source": [
    "## Plot surrogate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961026e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ PLOT PREDICTIONS ############################ \n",
    "n_train = Xnorm.shape[0]\n",
    "n_rt = np.sqrt(n_train).astype(int)\n",
    "X = np.zeros((n_rt, n_rt))\n",
    "Y = np.zeros((n_rt, n_rt))\n",
    "Z_true = np.zeros((n_rt, n_rt))\n",
    "Z_pred = np.zeros((n_rt, n_rt))\n",
    "for i in range(n_rt):\n",
    "    for j in range(n_rt):\n",
    "        X[i, j] = Xnorm[i * n_rt + j, 0]\n",
    "        Y[i, j] = Xnorm[i * n_rt + j, 1]\n",
    "        Z_true[i, j] = ynorm[i * n_rt + j]\n",
    "        Z_pred[i, j] = y_pred[i * n_rt + j]\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "cmap = plt.get_cmap('coolwarm')\n",
    "ax.plot_surface(X, Y, Z_true, cmap=cmap)\n",
    "ax.set_title('training data')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax.plot_surface(X, Y, Z_pred)\n",
    "ax.set_title('learned function')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4061345",
   "metadata": {},
   "source": [
    "## Optimize surrogate model with maingopy\n",
    "\n",
    "Melon computes the relaxations of the feed forward network ````maingopy.melonpy.FeedForwardNet()```` needed by the B&B solver by providing envelopes of the activation functions (currently supported: purelin, linear, tanh, tansig, relu, relu6). The procedure is the same with gaussian processes and support vector machines.\n",
    "\n",
    "We load the trained model information from the generated XML file (see utils for details). which will be the optimization problem parameters. Then, we define the problem bounds of $D$ (see problem definition) in maingopy. Lastly, we define the evaluation function for maingopy.\n",
    "\n",
    "Maingopy then takes the envelopes of the ANN and performs a global deterministic optimization (it is  B&B solver with a reduced space formulation for these envelopes, speeding up computation: https://www.avt.rwth-aachen.de/global/show_document.asp?id=aaaaaaaaabclahw). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To define a model, we need to spcecialize the MAiNGOmodel class\n",
    "class Model(maingopy.MAiNGOmodel):\n",
    "    def __init__(self):\n",
    "        maingopy.MAiNGOmodel.__init__(self)\n",
    "        # Initialize feedforward neural network and load data from example csv file\n",
    "        self.ffANN = maingopy.melonpy.FeedForwardNet()\n",
    "        # folder where the model xml is stored\n",
    "        self.path = \"./data/Output/\"\n",
    "        # xml filename\n",
    "        self.fileName= \"peaks\"\n",
    "        # open them (define that it is an XML instead of a CSV)\n",
    "        self.ffANN.load_model(self.path, self.fileName, maingopy.melonpy.XML)\n",
    "\n",
    "    # We need to implement the get_variables functions for specifying the optimization varibles\n",
    "    def get_variables(self):\n",
    "        # define bounds of the original variables, so that it rescales the results fo the optimization\n",
    "        # the optimization is done with the normalized version of these values\n",
    "        variables = [ maingopy.OptimizationVariable(maingopy.Bounds(-3,3), maingopy.VT_CONTINUOUS, \"x\"),\n",
    "                      maingopy.OptimizationVariable(maingopy.Bounds(-3,3), maingopy.VT_CONTINUOUS, \"y\") ]\n",
    "        return variables\n",
    "\n",
    "    # We need to implement the evaluate function that computes the values of the objective and constraints from the variables.\n",
    "    # Note that the variables in the 'vars' argument of this function do correspond to the optimization variables defined in the get_variables function.\n",
    "    # However, they are different objects for technical reasons. The only mapping we have between them is the position in the list.\n",
    "    # The results of the evaluation (i.e., objective and constraint values) need to be return in an EvaluationContainer\n",
    "    def evaluate(self, vars):\n",
    "        x = vars[0]\n",
    "        y = vars[1]\n",
    "        \n",
    "        # Inputs to the ANN are the variables x and y\n",
    "        annInputs = [x, y]\n",
    "        \n",
    "        # Evaluate the network (in reduced-space)\n",
    "        # This returns a list, because the output of the network may be multidimensional\n",
    "        annOutputs = self.ffANN.calculate_prediction_reduced_space(annInputs)\n",
    "\n",
    "        # Set the ANN output (only 1 in this case) as objective to be minimized\n",
    "        result = maingopy.EvaluationContainer()\n",
    "        result.objective = annOutputs[0]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To work with the problem, we first create an instance of the model.\n",
    "myModel = Model()\n",
    "# We then create an instance of MAiNGO, the solver, and hand it the model.\n",
    "myMAiNGO = maingopy.MAiNGO(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd6df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can have MAiNGO read a settings file:\n",
    "#fileName = \"\"\n",
    "#myMAiNGO.read_settings(fileName) # If fileName is empty, MAiNGO will attempt to open MAiNGOSettings.txt\n",
    "myMAiNGO.set_log_file_name(\".logs/my_log_file.log\")\n",
    "myMAiNGO.set_option(\"writeCsv\", True)\n",
    "myMAiNGO.set_iterations_csv_file_name(\".logs/iterations.csv\")\n",
    "myMAiNGO.set_solution_and_statistics_csv_file_name(\".logs/solution_and_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb8d0a",
   "metadata": {},
   "source": [
    "You can see the input in form of a GAMS file in case you are interested in ````./logs/my_problem_file_MAiNGO.gms````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "myMAiNGO.write_model_to_file_in_other_language(writingLanguage=maingopy.LANG_GAMS, fileName=\"./logs/my_problem_file_MAiNGO.gms\", solverName=\"SCIP\", writeRelaxationOnly=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f73e6",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bd166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we call the solve routine to solve the problem.\n",
    "maingoStatus = myMAiNGO.solve()\n",
    "print(maingoStatus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f213eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global optimum: f([{}, {}]) = {}\".format(myMAiNGO.get_solution_point()[0], myMAiNGO.get_solution_point()[1], myMAiNGO.get_objective_value()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANN value at true minimum = g(0.228, -1.626) = {}\".format(myMAiNGO.evaluate_model_at_point(np.array([0.228, -1.626]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea2656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
